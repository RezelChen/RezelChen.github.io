<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/blog/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/blog/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/blog/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/blog/images/logo.svg" color="#222">

<link rel="stylesheet" href="/blog/css/main.css">


<link rel="stylesheet" href="/blog/lib/font-awesome/css/all.min.css">
  
  <link rel="stylesheet" href="/blog/lib/animate-css/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"rezelchen.github.io","root":"/blog/","scheme":"Muse","version":"8.0.0-rc.4","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Ray Chen&#39;s Blog">
<meta property="og:url" content="https://rezelchen.github.io/index.html">
<meta property="og:site_name" content="Ray Chen&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Ray Chen">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://rezelchen.github.io/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Ray Chen's Blog</title>
  






  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <main class="main">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader">
        <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
    </div>
  </div>

  <div class="site-meta">

    <a href="/blog/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Ray Chen's Blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>







</div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <section class="post-toc-wrap sidebar-panel">
      </section>
      <!--/noindex-->

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ray Chen</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/blog/archives">
          <span class="site-state-item-count">10</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </section>
    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </header>

      
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div id="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


      <div class="main-inner">
        

        <div class="content index posts-expand">
          
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://rezelchen.github.io/2020/08/18/DDIA10/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/images/avatar.gif">
      <meta itemprop="name" content="Ray Chen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ray Chen's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/2020/08/18/DDIA10/" class="post-title-link" itemprop="url">Note for DDIA in Chapter 10</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-08-18 18:23:00" itemprop="dateCreated datePublished" datetime="2020-08-18T18:23:00+00:00">2020-08-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-08-20 07:33:40" itemprop="dateModified" datetime="2020-08-20T07:33:40+00:00">2020-08-20</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Batch-Processing"><a href="#Batch-Processing" class="headerlink" title="Batch Processing"></a>Batch Processing</h2><p>Let’s distinguish three different types of systems:</p>
<ul>
<li>Services (online systems)</li>
<li>Batch processing systems (offline systems)</li>
<li>Stream processing systems (near-real-time systems)</li>
</ul>
<h3 id="Batch-Processing-with-Unix-Tools"><a href="#Batch-Processing-with-Unix-Tools" class="headerlink" title="Batch Processing with Unix Tools"></a>Batch Processing with Unix Tools</h3><h4 id="Simple-Log-Analysis"><a href="#Simple-Log-Analysis" class="headerlink" title="Simple Log Analysis"></a>Simple Log Analysis</h4><h5 id="Chain-of-commands-versus-custom-program"><a href="#Chain-of-commands-versus-custom-program" class="headerlink" title="Chain of commands versus custom program"></a>Chain of commands versus custom program</h5><h5 id="Sorting-versus-in-memory-aggregation"><a href="#Sorting-versus-in-memory-aggregation" class="headerlink" title="Sorting versus in-memory aggregation"></a>Sorting versus in-memory aggregation</h5><h4 id="The-Unix-Philosophy"><a href="#The-Unix-Philosophy" class="headerlink" title="The Unix Philosophy"></a>The Unix Philosophy</h4><p>The philosophy was described in 1978 as follows:</p>
<ol>
<li><p>Make each program do one thing well. </p>
</li>
<li><p>Expect the output of every program to become the input to another, as yet unknown, program.</p>
</li>
<li><p>Design and build software, even operating systems, to be tried early, ideally within weeks. Don’t hesitate to throw away the clumsy parts and rebuild them.</p>
</li>
<li><p>Use tools in preference to unskilled help to lighten a programming task, even if you have to detour to build the tools and expect to throw some of them out after you’ve finished using them.</p>
</li>
</ol>
<h5 id="A-uniform-interface"><a href="#A-uniform-interface" class="headerlink" title="A uniform interface"></a>A uniform interface</h5><p>If you expect the output of one program to become the input to another program, that means those programs must use the same data format -— in other words, a compatible interface. If you want to be able to connect <em>any</em> program’s output to any pro‐ gram’s input, that means that <em>all</em> programs must use the same input/output interface.</p>
<p>In Unix, that interface is a file (or, more precisely, a file descriptor).</p>
<p>Today it’s an exception, not the norm, to have programs that work together as smoothly as Unix tools do.</p>
<h5 id="Separation-of-logic-and-wiring"><a href="#Separation-of-logic-and-wiring" class="headerlink" title="Separation of logic and wiring"></a>Separation of logic and wiring</h5><p>Another characteristic feature of Unix tools is their use of standard input (<code>stdin</code>) and standard output (<code>stdout</code>). </p>
<p>You can even write your own programs and combine them with the tools provided by the operating system. Your program just needs to read input from <code>stdin</code> and write output to <code>stdout</code>, and it can participate in data processing pipelines. </p>
<p>However, there are limits to what you can do with <code>stdin</code> and <code>stdout</code>. Programs that need multiple inputs or outputs are possible but tricky.</p>
<h5 id="Transparency-and-experimentation"><a href="#Transparency-and-experimentation" class="headerlink" title="Transparency and experimentation"></a>Transparency and experimentation</h5><p>Part of what makes Unix tools so successful is that they make it quite easy to see what is going on:</p>
<ul>
<li><p>The input files to Unix commands are normally treated as immutable. This means you can run the commands as often as you want, trying various command-line options, without damaging the input files.</p>
</li>
<li><p>You can end the pipeline at any point, pipe the output into less, and look at it to see if it has the expected form. This ability to inspect is great for debugging.</p>
</li>
<li><p>You can write the output of one pipeline stage to a file and use that file as input to the next stage. This allows you to restart the later stage without rerunning the entire pipeline.</p>
</li>
</ul>
<p>However, the biggest limitation of Unix tools is that they run only on a single machine—and that’s where tools like Hadoop come in.</p>
<h3 id="MapReduce-and-Distributed-Filesystems"><a href="#MapReduce-and-Distributed-Filesystems" class="headerlink" title="MapReduce and Distributed Filesystems"></a>MapReduce and Distributed Filesystems</h3><p>A single MapReduce job is comparable to a single Unix process: it takes one or more inputs and produces one or more outputs.</p>
<p>As with most Unix tools, running a MapReduce job normally does not modify the input and does not have any side effects other than producing the output. </p>
<h4 id="MapReduce-Job-Execution"><a href="#MapReduce-Job-Execution" class="headerlink" title="MapReduce Job Execution"></a>MapReduce Job Execution</h4><p>The pattern of data processing in MapReduce:</p>
<ol>
<li><p>Read a set of input files, and break it up into <em>records</em>. </p>
</li>
<li><p>Call the mapper function to extract a key and value from each input record. </p>
</li>
<li><p>Sort all of the key-value pairs by key.</p>
</li>
<li><p>Call the reducer function to iterate over the sorted key-value pairs. If there are multiple occurrences of the same key, the sorting has made them adjacent in the list, so it is easy to combine those values without having to keep a lot of state in memory. </p>
</li>
</ol>
<p>Viewed like this, the role of the mapper is to prepare the data by putting it into a form that is suitable for sorting, and the role of the reducer is to process the data that has been sorted.</p>
<h5 id="Distributed-execution-of-MapReduce"><a href="#Distributed-execution-of-MapReduce" class="headerlink" title="Distributed execution of MapReduce"></a>Distributed execution of MapReduce</h5><p>The MapReduce scheduler (not shown in the diagram) tries to run each mapper on one of the machines that stores a replica of the input file, provided that machine has enough spare RAM and CPU resources to run the map task. This principle is known as <em>putting the computation near the data</em>: it saves copying the input file over the network, reducing network load and increasing locality.</p>
<p>The reduce side of the computation is also partitioned. While the number of map tasks is determined by the number of input file blocks, the number of reduce tasks is configured by the job author (it can be different from the number of map tasks).</p>
<p>The key-value pairs must be sorted, but the dataset is likely too large to be sorted with a conventional sorting algorithm on a single machine. Instead, the sorting is per‐ formed in stages. First, each map task partitions its output by reducer, based on the hash of the key. Each of these partitions is written to a sorted file on the mapper’s local disk, using a technique similar to what we discussed in “SSTables and LSM- Trees” on page 76.</p>
<p>The process of partitioning by reducer, sorting, and copying data partitions from mappers to reducers is known as the <em>shuffle</em>.</p>
<p>The reduce task takes the files from the mappers and merges them together, preserv‐ ing the sort order. </p>
<p>The reducer is called with a key and an iterator that incrementally scans over all records with the same key.</p>
<h5 id="MapReduce-workflows"><a href="#MapReduce-workflows" class="headerlink" title="MapReduce workflows"></a>MapReduce workflows</h5><p>It is very common for MapReduce jobs to be chained together into workflows, such that the output of one job becomes the input to the next job. </p>
<p>A batch job’s output is only considered valid when the job has completed successfully. Therefore, one job in a workflow can only start when the prior jobs -— that is, the jobs that produce its input directories —- have completed successfully.</p>
<p>These schedulers also have management features that are useful when maintaining a large collection of batch jobs.</p>
<h4 id="Reduce-Side-Joins-and-Grouping"><a href="#Reduce-Side-Joins-and-Grouping" class="headerlink" title="Reduce-Side Joins and Grouping"></a>Reduce-Side Joins and Grouping</h4><p>When we talk about joins in the context of batch processing, we mean resolving all occurrences of some association within a dataset. For example, we assume that a job is processing the data for all users simultaneously, not merely looking up the data for one particular user (which would be done far more efficiently with an index).</p>
<h5 id="Example-analysis-of-user-activity-events"><a href="#Example-analysis-of-user-activity-events" class="headerlink" title="Example: analysis of user activity events"></a>Example: analysis of user activity events</h5><h5 id="Sort-merge-joins"><a href="#Sort-merge-joins" class="headerlink" title="Sort-merge joins"></a>Sort-merge joins</h5><h5 id="Bringing-related-data-together-in-the-same-place"><a href="#Bringing-related-data-together-in-the-same-place" class="headerlink" title="Bringing related data together in the same place"></a>Bringing related data together in the same place</h5><p>In a sort-merge join, the mappers and the sorting process make sure that all the nec‐ essary data to perform the join operation for a particular user ID is brought together in the same place: a single call to the reducer. </p>
<p>One way of looking at this architecture is that mappers “send messages” to the reduc‐ ers. When a mapper emits a key-value pair, the key acts like the destination address to which the value should be delivered. Even though the key is just an arbitrary string (not an actual network address like an IP address and port number), it behaves like an address: all key-value pairs with the same key will be delivered to the same destination (a call to the reducer).</p>
<h5 id="GROUP-BY"><a href="#GROUP-BY" class="headerlink" title="GROUP BY"></a>GROUP BY</h5><h5 id="Handling-skew"><a href="#Handling-skew" class="headerlink" title="Handling skew"></a>Handling skew</h5><p>The pattern of “bringing all records with the same key to the same place” breaks down if there is a very large amount of data related to a single key. </p>
<p>When performing the actual join, the mappers send any records relating to a hot key to one of several reducers, chosen at random (in contrast to conventional MapReduce, which chooses a reducer deterministically based on a hash of the key). For the other input to the join, records relating to the hot key need to be replicated to all reducers handling that key.</p>
<h4 id="Map-Side-Joins"><a href="#Map-Side-Joins" class="headerlink" title="Map-Side Joins"></a>Map-Side Joins</h4><p>The join algorithms described in the last section perform the actual join logic in the reducers, and are hence known as reduce-side joins. The mappers take the role of preparing the input data: extracting the key and value from each input record, assigning the key-value pairs to a reducer partition, and sorting by key.</p>
<h5 id="Broadcast-hash-joins"><a href="#Broadcast-hash-joins" class="headerlink" title="Broadcast hash joins"></a>Broadcast hash joins</h5><p>The simplest way of performing a map-side join applies in the case where a large dataset is joined with a small dataset. In particular, the small dataset needs to be small enough that it can be loaded entirely into memory in each of the mappers.</p>
<p>This simple but effective algorithm is called a <em>broadcast hash join</em>: the word broadcast reflects the fact that each mapper for a partition of the large input reads the entirety of the small input (so the small input is effectively “broadcast” to all partitions of the large input), and the word hash reflects its use of a hash table. </p>
<h5 id="Partitioned-hash-joins"><a href="#Partitioned-hash-joins" class="headerlink" title="Partitioned hash joins"></a>Partitioned hash joins</h5><p>If the inputs to the map-side join are partitioned in the same way, then the hash join approach can be applied to each partition independently. </p>
<p>This approach only works if both of the join’s inputs have the same number of partitions, with records assigned to partitions based on the same key and the same hash function. If the inputs are generated by prior MapReduce jobs that already perform this grouping, then this can be a reasonable assumption to make.</p>
<h5 id="Map-side-merge-joins"><a href="#Map-side-merge-joins" class="headerlink" title="Map-side merge joins"></a>Map-side merge joins</h5><p>Another variant of a map-side join applies if the input datasets are not only parti‐ tioned in the same way, but also <em>sorted</em> based on the same key. </p>
<p>If a map-side merge join is possible, it probably means that prior MapReduce jobs brought the input datasets into this partitioned and sorted form in the first place. In principle, this join could have been performed in the reduce stage of the prior job. However, it may still be appropriate to perform the merge join in a separate map- only job, for example if the partitioned and sorted datasets are also needed for other purposes besides this particular join.</p>
<h5 id="MapReduce-workflows-with-map-side-joins"><a href="#MapReduce-workflows-with-map-side-joins" class="headerlink" title="MapReduce workflows with map-side joins"></a>MapReduce workflows with map-side joins</h5><p>When the output of a MapReduce join is consumed by downstream jobs, the choice of map-side or reduce-side join affects the structure of the output. The output of a reduce-side join is partitioned and sorted by the join key, whereas the output of a map-side join is partitioned and sorted in the same way as the large input.</p>
<h4 id="The-Output-of-Batch-Workflows"><a href="#The-Output-of-Batch-Workflows" class="headerlink" title="The Output of Batch Workflows"></a>The Output of Batch Workflows</h4><p>The output of a batch process is often not a report, but some other kind of structure.</p>
<h5 id="Building-search-indexes"><a href="#Building-search-indexes" class="headerlink" title="Building search indexes"></a>Building search indexes</h5><p>If you need to perform a full-text search over a fixed set of documents, then a batch process is a very effective way of building the indexes: the mappers partition the set of documents as needed, each reducer builds the index for its partition, and the index files are written to the distributed filesystem. </p>
<h5 id="Key-value-stores-as-batch-process-output"><a href="#Key-value-stores-as-batch-process-output" class="headerlink" title="Key-value stores as batch process output"></a>Key-value stores as batch process output</h5><p>Another common use for batch processing is to build machine learning systems such as classifiers (e.g., spam filters, anomaly detection, image recognition) and recommendation systems (e.g., people you may know, products you may be interested in, or related searches).</p>
<p>A much better solution is to build a brand-new database <em>inside</em> the batch job and write it as files to the job’s output directory in the distributed filesystem, just like the search indexes in the last section. Those data files are then immutable once written, and can be loaded in bulk into servers that handle read-only queries. Various key- value stores support building database files in MapReduce jobs, including Voldemort, Terrapin, ElephantDB, and HBase bulk loading.</p>
<h5 id="Philosophy-of-batch-process-outputs"><a href="#Philosophy-of-batch-process-outputs" class="headerlink" title="Philosophy of batch process outputs"></a>Philosophy of batch process outputs</h5><h4 id="Comparing-Hadoop-to-Distributed-Databases"><a href="#Comparing-Hadoop-to-Distributed-Databases" class="headerlink" title="Comparing Hadoop to Distributed Databases"></a>Comparing Hadoop to Distributed Databases</h4><h5 id="Diversity-of-storage"><a href="#Diversity-of-storage" class="headerlink" title="Diversity of storage"></a>Diversity of storage</h5><p>…</p>
<h3 id="Beyond-MapReduce"><a href="#Beyond-MapReduce" class="headerlink" title="Beyond MapReduce"></a>Beyond MapReduce</h3><p>In response to the difficulty of using MapReduce directly, various higher-level pro‐ gramming models (Pig, Hive, Cascading, Crunch) were created as abstractions on top of MapReduce.</p>
<p>In the rest of this chapter, we will look at some of those alternatives for batch processing. In Chapter 11 we will move to stream processing, which can be regarded as another way of speeding up batch processing.</p>
<h4 id="Materialization-of-Intermediate-State"><a href="#Materialization-of-Intermediate-State" class="headerlink" title="Materialization of Intermediate State"></a>Materialization of Intermediate State</h4><p>Pipes do not fully materialize the intermediate state, but instead <em>stream</em> the output to the input incrementally, using only a small in-memory buffer.</p>
<p>MapReduce’s approach of fully materializing intermediate state has downsides com‐ pared to Unix pipes:</p>
<ul>
<li><p>A MapReduce job can only start when all tasks in the preceding jobs have completed, whereas processes connected by a Unix pipe are started at the same time, with output being consumed as soon as it is produced.</p>
</li>
<li><p>Mappers are often redundant: they just read back the same file that was just writ‐ ten by a reducer, and prepare it for the next stage of partitioning and sorting. </p>
</li>
<li><p>Storing intermediate state in a distributed filesystem means those files are repli‐ cated across several nodes, which is often overkill for such temporary data.</p>
</li>
</ul>
<h5 id="Dataflow-engines"><a href="#Dataflow-engines" class="headerlink" title="Dataflow engines"></a>Dataflow engines</h5><p>In order to fix these problems with MapReduce, several new execution engines for distributed batch computations were developed, the most well known of which are Spark, Tez, and Flink. They handle an entire workflow as one job, rather than breaking it up into independent subjobs.</p>
<p>Since they explicitly model the flow of data through several processing stages, these systems are known as <em>dataflow engines</em>.</p>
<p>You can use dataflow engines to implement the same computations as MapReduce workflows, and they usually execute significantly faster due to the optimizations described here. </p>
<h5 id="Fault-tolerance"><a href="#Fault-tolerance" class="headerlink" title="Fault tolerance"></a>Fault tolerance</h5><p>Spark, Flink, and Tez avoid writing intermediate state to HDFS, so they take a different approach to tolerating faults: if a machine fails and the intermediate state on that machine is lost, it is recomputed from other data that is still available (a prior inter‐ mediary stage if possible, or otherwise the original input data, which is normally on HDFS).</p>
<p>Recovering from faults by recomputing data is not always the right answer: if the intermediate data is much smaller than the source data, or if the computation is very CPU-intensive, it is probably cheaper to materialize the intermediate data to files than to recompute it.</p>
<h5 id="Discussion-of-materialization"><a href="#Discussion-of-materialization" class="headerlink" title="Discussion of materialization"></a>Discussion of materialization</h5><h4 id="Graphs-and-Iterative-Processing"><a href="#Graphs-and-Iterative-Processing" class="headerlink" title="Graphs and Iterative Processing"></a>Graphs and Iterative Processing</h4><p>…</p>
<h4 id="High-Level-APIs-and-Languages"><a href="#High-Level-APIs-and-Languages" class="headerlink" title="High-Level APIs and Languages"></a>High-Level APIs and Languages</h4><h5 id="The-move-toward-declarative-query-languages"><a href="#The-move-toward-declarative-query-languages" class="headerlink" title="The move toward declarative query languages"></a>The move toward declarative query languages</h5><p>An advantage of specifying joins as relational operators, compared to spelling out the code that performs the join, is that the framework can analyze the properties of the join inputs and automatically decide which of the aforementioned join algorithms would be most suitable for the task at hand.</p>
<p>By incorporating declarative aspects in their high-level APIs, and having query opti‐ mizers that can take advantage of them during execution, batch processing frameworks begin to look more like MPP databases (and can achieve comparable performance). At the same time, by having the extensibility of being able to run arbitrary code and read data in arbitrary formats, they retain their flexibility advantage.</p>
<h5 id="Specialization-for-different-domains"><a href="#Specialization-for-different-domains" class="headerlink" title="Specialization for different domains"></a>Specialization for different domains</h5><p>Also useful are spatial algorithms such as <em>k-nearest neighbors</em>, which searches for items that are close to a given item in some multi-dimensional space—a kind of simi‐ larity search. Approximate search is also important for genome analysis algorithms, which need to find strings that are similar but not identical.</p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>…</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://rezelchen.github.io/2020/08/17/DDIA9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/images/avatar.gif">
      <meta itemprop="name" content="Ray Chen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ray Chen's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/2020/08/17/DDIA9/" class="post-title-link" itemprop="url">Note for DDIA in Chapter 9</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-08-17 23:39:00" itemprop="dateCreated datePublished" datetime="2020-08-17T23:39:00+00:00">2020-08-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-08-20 07:33:40" itemprop="dateModified" datetime="2020-08-20T07:33:40+00:00">2020-08-20</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Consistency-and-Consensus"><a href="#Consistency-and-Consensus" class="headerlink" title="Consistency and Consensus"></a>Consistency and Consensus</h2><p>The best way of building fault-tolerant systems is to find some general-purpose abstractions with useful guarantees, implement them once, and then let applications rely on those guarantees.</p>
<p>One of the most important abstractions for distributed systems is consensus: that is, getting all of the nodes to agree on something.</p>
<p>The limits of what is and isn’t possible have been explored in depth, both in theoretical proofs and in practical implementations. We will get an overview of those fundamental limits in this chapter.</p>
<h3 id="Consistency-Guarantees"><a href="#Consistency-Guarantees" class="headerlink" title="Consistency Guarantees"></a>Consistency Guarantees</h3><p>A better name for <em>eventual consistency</em> may be <em>convergence</em>, as we expect all replicas to eventually converge to the same value.</p>
<p>There is some similarity between distributed consistency models and the hierarchy of transaction isolation levels we discussed previously. But while there is some overlap, they are mostly independent con‐ cerns: transaction isolation is primarily about avoiding race conditions due to concurrently executing transactions, whereas distributed consistency is mostly about coordinating the state of replicas in the face of delays and faults.</p>
<p>This chapter covers a broad range of topics, but as we shall see, these areas are in fact deeply linked:</p>
<ul>
<li><p>We will start by looking at one of the strongest consistency models in common use, <em>linearizability</em>, and examine its pros and cons.</p>
</li>
<li><p>We’ll then examine the issue of ordering events in a distributed system, particularly around causality and total ordering.</p>
</li>
<li><p>In the third section we will explore how to atomically commit a distributed transaction, which will finally lead us toward solutions for the consensus problem.</p>
</li>
</ul>
<h3 id="Linearizability"><a href="#Linearizability" class="headerlink" title="Linearizability"></a>Linearizability</h3><p>The exact definition of linearizability is quite subtle, and we will explore it in the rest of this section. But the basic idea is to make a system appear as if there were only one copy of the data, and all operations on it are atomic. With this guarantee, even though there may be multiple replicas in reality, the application does not need to worry about them.</p>
<h4 id="What-Makes-a-System-Linearizable"><a href="#What-Makes-a-System-Linearizable" class="headerlink" title="What Makes a System Linearizable?"></a>What Makes a System Linearizable?</h4><p>The requirement of linearizability is that the lines joining up the operation markers always move forward in time (from left to right), never backward. This requirement ensures the recency guarantee we discussed earlier: once a new value has been written or read, all subsequent reads see the value that was written, until it is overwritten again.</p>
<p>Linearizability Versus Serializability: </p>
<ul>
<li><p>Serializability is an isolation property of transactions, where every transaction may read and write multiple objects. It guarantees that transactions behave the same as if they had executed in some serial order (each transaction running to completion before the next transaction starts). It is okay for that serial order to be different from the order in which transactions were actually run.</p>
</li>
<li><p>Linearizability is a recency guarantee on reads and writes of a register (an individual object). It doesn’t group operations together into transactions, so it does not prevent problems such as write skew, unless you take additional measures such as materializing conflicts.</p>
</li>
</ul>
<p>Implementations of serializability based on two-phase locking or actual serial execution are typically linearizable.</p>
<p>However, serializable snapshot isolation is not linearizable: by design, it makes reads from a consistent snapshot, to avoid lock contention between readers and writers. The whole point of a consistent snapshot is that it does not include writes that are more recent than the snapshot, and thus reads from the snapshot are not linearizable.</p>
<h4 id="Relying-on-Linearizability"><a href="#Relying-on-Linearizability" class="headerlink" title="Relying on Linearizability"></a>Relying on Linearizability</h4><h5 id="Locking-and-leader-election"><a href="#Locking-and-leader-election" class="headerlink" title="Locking and leader election"></a>Locking and leader election</h5><p>A system that uses single-leader replication needs to ensure that there is indeed only one leader, not several (split brain). One way of electing a leader is to use a lock: every node that starts up tries to acquire the lock, and the one that succeeds becomes the leader.</p>
<p>Distributed locking is also used at a much more granular level in some distributed databases, such as Oracle Real Application Clusters (RAC). RAC uses a lock per disk page, with multiple nodes sharing access to the same disk storage system.</p>
<h5 id="Constraints-and-uniqueness-guarantees"><a href="#Constraints-and-uniqueness-guarantees" class="headerlink" title="Constraints and uniqueness guarantees"></a>Constraints and uniqueness guarantees</h5><p>This situation is actually similar to a lock: when a user registers for your service, you can think of them acquiring a “lock” on their chosen username. The operation is also very similar to an atomic compare-and-set, setting the username to the ID of the user who claimed it, provided that the username is not already taken.</p>
<h5 id="Cross-channel-timing-dependencies"><a href="#Cross-channel-timing-dependencies" class="headerlink" title="Cross-channel timing dependencies"></a>Cross-channel timing dependencies</h5><h4 id="Implementing-Linearizable-Systems"><a href="#Implementing-Linearizable-Systems" class="headerlink" title="Implementing Linearizable Systems"></a>Implementing Linearizable Systems</h4><p>The most common approach to making a system fault-tolerant is to use replication. Let’s revisit the replication methods from Chapter 5, and compare whether they can be made linearizable:</p>
<ul>
<li>Single-leader replication (potentially linearizable)</li>
<li>Consensus algorithms (linearizable)</li>
<li>Multi-leader replication (not linearizable)</li>
<li>Leaderless replication (probably not linearizable)</li>
</ul>
<h5 id="Linearizability-and-quorums"><a href="#Linearizability-and-quorums" class="headerlink" title="Linearizability and quorums"></a>Linearizability and quorums</h5><p>It is possible to make Dynamo-style quorums linearizable at the cost of reduced performance: a reader must perform <em>read repair</em> synchronously, before returning results to the application, and a writer must read the latest state of a quorum of nodes before sending its writes.</p>
<p>Moreover, only linearizable read and write operations can be implemented in this way; a linearizable compare-and-set operation cannot, because it requires a consensus algorithm.</p>
<p>In summary, it is safest to assume that a leaderless system with Dynamo-style replication does not provide linearizability.</p>
<h4 id="The-Cost-of-Linearizability"><a href="#The-Cost-of-Linearizability" class="headerlink" title="The Cost of Linearizability"></a>The Cost of Linearizability</h4><h5 id="The-CAP-theorem"><a href="#The-CAP-theorem" class="headerlink" title="The CAP theorem"></a>The CAP theorem</h5><p>The trade-off is as follows:</p>
<ul>
<li><p>If your application <em>requires</em> linearizability, and some replicas are disconnected from the other replicas due to a network problem, then some replicas cannot process requests while they are disconnected: they must either wait until the network problem is fixed, or return an error (either way, they become <em>unavailable</em>).</p>
</li>
<li><p>If your application <em>does not require</em> linearizability, then it can be written in a way that each replica can process requests independently, even if it is disconnected from other replicas (e.g., multi-leader). In this case, the application can remain <em>available</em> in the face of a network problem, but its behavior is not linearizable.</p>
</li>
</ul>
<p>CAP is sometimes presented as <em>Consistency, Availability, Partition tolerance: pick 2 out of 3</em>. Unfortunately, putting it this way is misleading because network partitions are a kind of fault, so they aren’t something about which you have a choice: they will happen whether you like it or not.</p>
<p>Thus, a better way of phrasing CAP would be <em>either Consistent or Available when Partitioned</em>.</p>
<h5 id="Linearizability-and-network-delays"><a href="#Linearizability-and-network-delays" class="headerlink" title="Linearizability and network delays"></a>Linearizability and network delays</h5><p>Although linearizability is a useful guarantee, surprisingly few systems are actually linearizable in practice.</p>
<p>The reason for dropping linearizability is <em>performance</em>, not fault tolerance.</p>
<p>In Chapter 12 we will discuss some approaches for avoiding linearizability without sacrificing correctness.</p>
<h3 id="Ordering-Guarantees"><a href="#Ordering-Guarantees" class="headerlink" title="Ordering Guarantees"></a>Ordering Guarantees</h3><p>It turns out that there are deep connections between ordering, linearizability, and consensus. </p>
<h4 id="Ordering-and-Causality"><a href="#Ordering-and-Causality" class="headerlink" title="Ordering and Causality"></a>Ordering and Causality</h4><p>There are several reasons why ordering keeps coming up, and one of the reasons is that it helps preserve <em>causality</em>. </p>
<p>Causality imposes an ordering on events: cause comes before effect; a message is sent before that message is received; the question comes before the answer. </p>
<p>If a system obeys the ordering imposed by causality, we say that it is <em>causally consistent</em>. For example, snapshot isolation provides causal consistency: when you read from the database, and you see some piece of data, then you must also be able to see any data that causally precedes it.</p>
<h5 id="The-causal-order-is-not-a-total-order"><a href="#The-causal-order-is-not-a-total-order" class="headerlink" title="The causal order is not a total order"></a>The causal order is not a total order</h5><p>The difference between a <em>total order</em> and a <em>partial order</em> is reflected in different data‐ base consistency models:</p>
<ul>
<li><p>Linearizability: In a linearizable system, we have a total order of operations: if the system behaves as if there is only a single copy of the data, and every operation is atomic, this means that for any two operations we can always say which one happened first.</p>
</li>
<li><p>Causality: We said that two operations are concurrent if neither happened before the other. Put another way, two events are ordered if they are causally related (one happened before the other), but they are incomparable if they are concurrent. This means that causality defines a <em>partial order</em>, not a total order: some operations are ordered with respect to each other, but some are incomparable.</p>
</li>
</ul>
<p>If you are familiar with distributed version control systems such as Git, their version histories are very much like the graph of causal dependencies. Often one commit happens after another, in a straight line, but sometimes you get branches (when sev‐ eral people concurrently work on a project), and merges are created when those con‐ currently created commits are combined.</p>
<h5 id="Linearizability-is-stronger-than-causal-consistency"><a href="#Linearizability-is-stronger-than-causal-consistency" class="headerlink" title="Linearizability is stronger than causal consistency"></a>Linearizability is stronger than causal consistency</h5><p>Any system that is linearizable will preserve cau‐ sality correctly.</p>
<p>The good news is that a middle ground is possible. Linearizability is not the only way of preserving causality—there are other ways too. In fact, causal consistency is the strongest possible consistency model that does not slow down due to network delays, and remains available in the face of network failures.</p>
<p>In many cases, systems that appear to require linearizability in fact only really require causal consistency, which can be implemented more efficiently. Based on this obser‐ vation, researchers are exploring new kinds of databases that preserve causality, with performance and availability characteristics that are similar to those of eventually consistent systems.</p>
<h5 id="Capturing-causal-dependencies"><a href="#Capturing-causal-dependencies" class="headerlink" title="Capturing causal dependencies"></a>Capturing causal dependencies</h5><p>The techniques for determining which operation happened before which other oper‐ ation are similar to what we discussed in “Detecting Concurrent Writes” on page 184.</p>
<p>In order to determine the causal ordering, the database needs to know which version of the data was read by the application. This is why, in Figure 5-13, the version num‐ ber from the prior operation is passed back to the database on a write. A similar idea appears in the conflict detection of SSI, as discussed in “Serializable Snapshot Isolation (SSI)” on page 261: when a transaction wants to commit, the database checks whether the version of the data that it read is still up to date. To this end, the database keeps track of which data has been read by which transaction.</p>
<h4 id="Sequence-Number-Ordering"><a href="#Sequence-Number-Ordering" class="headerlink" title="Sequence Number Ordering"></a>Sequence Number Ordering</h4><p>Although causality is an important theoretical concept, actually keeping track of all causal dependencies can become impractical.</p>
<p>However, there is a better way: we can use <em>sequence numbers</em> or <em>timestamps</em> to order events. A timestamp need not come from a time-of-day clock (or physical clock, which have many problems, as discussed in “Unreliable Clocks” on page 287). It can instead come from a logical clock, which is an algorithm to generate a sequence of numbers to identify operations, typically using counters that are incremented for every operation.</p>
<p>In particular, we can create sequence numbers in a total order that is <em>consistent with causality</em>: we promise that if operation A causally happened before B, then A occurs before B in the total order.</p>
<p>In a database with single-leader replication (see “Leaders and Followers” on page 152), the replication log defines a total order of write operations that is consistent with causality. The leader can simply increment a counter for each operation, and thus assign a monotonically increasing sequence number to each operation in the replication log. If a follower applies the writes in the order they appear in the replication log, the state of the follower is always causally consistent (even if it is lagging behind the leader).</p>
<h5 id="Noncausal-sequence-number-generators"><a href="#Noncausal-sequence-number-generators" class="headerlink" title="Noncausal sequence number generators"></a>Noncausal sequence number generators</h5><h5 id="Lamport-timestamps"><a href="#Lamport-timestamps" class="headerlink" title="Lamport timestamps"></a>Lamport timestamps</h5><p>A Lamport timestamp bears no relationship to a physical time-of-day clock, but it provides total ordering: if you have two timestamps, the one with a greater counter value is the greater timestamp; if the counter values are the same, the one with the greater node ID is the greater timestamp.</p>
<p>The key idea about Lamport timestamps, which makes them consis‐ tent with causality, is the following: every node and every client keeps track of the <em>maximum</em> counter value it has seen so far, and includes that maximum on every request. When a node receives a request or response with a maximum counter value greater than its own counter value, it immediately increases its own counter to that maximum.</p>
<p>As long as the maximum counter value is carried along with every operation, this scheme ensures that the ordering from the Lamport timestamps is consistent with causality, because every causal dependency results in an increased timestamp.</p>
<p>Lamport timestamps are sometimes confused with version vectors, which we saw in “Detecting Concurrent Writes” on page 184. Although there are some similarities, they have a different purpose: version vectors can distinguish whether two operations are concurrent or whether one is causally dependent on the other, whereas Lamport timestamps always enforce a total ordering. </p>
<p>From the total ordering of Lamport timestamps, you cannot tell whether two operations are concurrent or whether they are causally dependent. The advantage of Lamport timestamps over version vectors is that they are more compact.</p>
<h5 id="Timestamp-ordering-is-not-sufficient"><a href="#Timestamp-ordering-is-not-sufficient" class="headerlink" title="Timestamp ordering is not sufficient"></a>Timestamp ordering is not sufficient</h5><p>The problem here is that the total order of operations only emerges after you have collected all of the operations. </p>
<p>If you have an operation to create a username, and you are sure that no other node can insert a claim for the same username ahead of your operation in the total order, then you can safely declare the operation successful. This idea of knowing when your total order is finalized is captured in the topic of <em>total order broadcast</em>.</p>
<h4 id="Total-Order-Broadcast"><a href="#Total-Order-Broadcast" class="headerlink" title="Total Order Broadcast"></a>Total Order Broadcast</h4><p>Total order broadcast is usually described as a protocol for exchanging messages between nodes. Informally, it requires that two safety properties always be satisfied:</p>
<ul>
<li><p>Reliable delivery: No messages are lost: if a message is delivered to one node, it is delivered to all nodes.</p>
</li>
<li><p>Totally ordered delivery: Messages are delivered to every node in the same order.</p>
</li>
</ul>
<h5 id="Using-total-order-broadcast"><a href="#Using-total-order-broadcast" class="headerlink" title="Using total order broadcast"></a>Using total order broadcast</h5><p>Consensus services such as ZooKeeper and etcd actually implement total order broadcast. This fact is a hint that there is a strong connection between total order broadcast and consensus, which we will explore later in this chapter.</p>
<p>An important aspect of total order broadcast is that the order is fixed at the time the messages are delivered.</p>
<p>Another way of looking at total order broadcast is that it is a way of creating a log (as in a replication log, transaction log, or write-ahead log): delivering a message is like appending to the log. Since all nodes must deliver the same messages in the same order, all nodes can read the log and see the same sequence of messages.</p>
<p>Total order broadcast is also useful for implementing a lock service that provides <em>fencing tokens</em>. Every request to acquire the lock is appended as a message to the log, and all messages are sequentially numbered in the order they appear in the log. The sequence number can then serve as a fencing token, because it is monotonically increasing. </p>
<h5 id="Implementing-linearizable-storage-using-total-order-broadcast"><a href="#Implementing-linearizable-storage-using-total-order-broadcast" class="headerlink" title="Implementing linearizable storage using total order broadcast"></a>Implementing linearizable storage using total order broadcast</h5><p>Total order broadcast is asynchronous: messages are guaranteed to be delivered relia‐ bly in a fixed order, but there is no guarantee about when a message will be delivered (so one recipient may lag behind the others). By contrast, linearizability is a recency guarantee: a read is guaranteed to see the latest value written.</p>
<p>You can implement such a linearizable compare-and-set operation as follows by using total order broadcast as an append-only log:</p>
<ol>
<li><p>Append a message to the log, tentatively indicating the username you want to claim.</p>
</li>
<li><p>Read the log, and wait for the message you appended to be delivered back to you.</p>
</li>
<li><p>Check for any messages claiming the username that you want. If the first message for your desired username is your own message, then you are successful; otherwise, you abort the operation.</p>
</li>
</ol>
<h5 id="Implementing-total-order-broadcast-using-linearizable-storage"><a href="#Implementing-total-order-broadcast-using-linearizable-storage" class="headerlink" title="Implementing total order broadcast using linearizable storage"></a>Implementing total order broadcast using linearizable storage</h5><p>The algorithm is simple: for every message you want to send through total order broadcast, you increment-and-get the linearizable integer, and then attach the value you got from the register as a sequence number to the message. You can then send the message to all nodes (resending any lost messages), and the recipients will deliver the messages consecutively by sequence number.</p>
<p>Note that unlike Lamport timestamps, the numbers you get from incrementing the linearizable register form a sequence with no gaps. Thus, if a node has delivered mes‐ sage 4 and receives an incoming message with a sequence number of 6, it knows that it must wait for message 5 before it can deliver message 6. The same is not the case with Lamport timestamps -— in fact, this is the key difference between total order broadcast and timestamp ordering.</p>
<p>It can be proved that a linearizable compare-and-set (or increment-and-get) register and total order broadcast are both equivalent to consensus.</p>
<h3 id="Distributed-Transactions-and-Consensus"><a href="#Distributed-Transactions-and-Consensus" class="headerlink" title="Distributed Transactions and Consensus"></a>Distributed Transactions and Consensus</h3><h4 id="Atomic-Commit-and-Two-Phase-Commit-2PC"><a href="#Atomic-Commit-and-Two-Phase-Commit-2PC" class="headerlink" title="Atomic Commit and Two-Phase Commit (2PC)"></a>Atomic Commit and Two-Phase Commit (2PC)</h4><h5 id="From-single-node-to-distributed-atomic-commit"><a href="#From-single-node-to-distributed-atomic-commit" class="headerlink" title="From single-node to distributed atomic commit"></a>From single-node to distributed atomic commit</h5><p>A node must only commit once it is certain that all other nodes in the transaction are also going to commit.</p>
<h5 id="Introduction-to-two-phase-commit"><a href="#Introduction-to-two-phase-commit" class="headerlink" title="Introduction to two-phase commit"></a>Introduction to two-phase commit</h5><p>2PC uses a new component that does not normally appear in single-node transactions: a coordinator (also known as transaction manager). The coordinator is often implemented as a library within the same application process that is requesting the transaction, but it can also be a separate pro‐ cess or service.</p>
<p>A 2PC transaction begins with the application reading and writing data on multiple database nodes, as normal. We call these database nodes participants in the transac‐ tion. When the application is ready to commit, the coordinator begins phase 1: it sends a prepare request to each of the nodes, asking them whether they are able to commit. The coordinator then tracks the responses from the participants:</p>
<ul>
<li><p>If all participants reply “yes,” indicating they are ready to commit, then the coor‐ dinator sends out a commit request in phase 2, and the commit actually takes place.</p>
</li>
<li><p>If any of the participants replies “no,” the coordinator sends an abort request to all nodes in phase 2.</p>
</li>
</ul>
<h5 id="A-system-of-promises"><a href="#A-system-of-promises" class="headerlink" title="A system of promises"></a>A system of promises</h5><p>Thus, the protocol contains two crucial “points of no return”: when a participant votes “yes,” it promises that it will definitely be able to commit later (although the coordinator may still choose to abort); and once the coordinator decides, that deci‐ sion is irrevocable. Those promises ensure the atomicity of 2PC.</p>
<h5 id="Coordinator-failure"><a href="#Coordinator-failure" class="headerlink" title="Coordinator failure"></a>Coordinator failure</h5><p>We have discussed what happens if one of the participants or the network fails during 2PC: if any of the prepare requests fail or time out, the coordinator aborts the trans‐ action; if any of the commit or abort requests fail, the coordinator retries them indefi‐ nitely. </p>
<h5 id="Three-phase-commit"><a href="#Three-phase-commit" class="headerlink" title="Three-phase commit"></a>Three-phase commit</h5><p>Two-phase commit is called a <em>blocking</em> atomic commit protocol due to the fact that 2PC can become stuck waiting for the coordinator to recover. In theory, it is possible to make an atomic commit protocol <em>nonblocking</em>, so that it does not get stuck if a node fails. However, making this work in practice is not so straightforward.</p>
<h4 id="Distributed-Transactions-in-Practice"><a href="#Distributed-Transactions-in-Practice" class="headerlink" title="Distributed Transactions in Practice"></a>Distributed Transactions in Practice</h4><h5 id="Exactly-once-message-processing"><a href="#Exactly-once-message-processing" class="headerlink" title="Exactly-once message processing"></a>Exactly-once message processing</h5><h5 id="XA-transactions"><a href="#XA-transactions" class="headerlink" title="XA transactions"></a>XA transactions</h5><p><em>X/Open XA</em> (short for eXtended Architecture) is a standard for implementing two- phase commit across heterogeneous technologies.</p>
<h5 id="Holding-locks-while-in-doubt"><a href="#Holding-locks-while-in-doubt" class="headerlink" title="Holding locks while in doubt"></a>Holding locks while in doubt</h5><h5 id="Recovering-from-coordinator-failure"><a href="#Recovering-from-coordinator-failure" class="headerlink" title="Recovering from coordinator failure"></a>Recovering from coordinator failure</h5><p>Many XA implementations have an emergency escape hatch called <em>heuristic decisions</em>: allowing a participant to unilaterally decide to abort or commit an in-doubt transaction without a definitive decision from the coordinator. To be clear, <em>heuristic</em> here is a euphemism for <em>probably breaking atomicity</em>, since it violates the system of promises in two-phase commit. </p>
<h5 id="Limitations-of-distributed-transactions"><a href="#Limitations-of-distributed-transactions" class="headerlink" title="Limitations of distributed transactions"></a>Limitations of distributed transactions</h5><p>XA transactions solve the real and important problem of keeping several participant data systems consistent with each other, but as we have seen, they also introduce major operational problems. In particular, the key realization is that the transaction coordinator is itself a kind of database (in which transaction outcomes are stored), and so it needs to be approached with the same care as any other important database:</p>
<ul>
<li><p>If the coordinator is not replicated but runs only on a single machine, it is a sin‐ gle point of failure for the entire system.</p>
</li>
<li><p>Many server-side applications are developed in a stateless model, with all persistent state stored in a database, which has the advantage that application servers can be added and removed at will. However, when the coordinator is part of the application server, it changes the nature of the deployment. Suddenly, the coordinator’s logs become a crucial part of the durable system state—as important as the databases themselves, since the coordinator logs are required in order to recover in-doubt transactions after a crash. Such application servers are no longer stateless.</p>
</li>
<li><p>Since XA needs to be compatible with a wide range of data systems, it is necessarily a lowest common denominator.</p>
</li>
<li><p>For database-internal distributed transactions (not XA), the limitations are not so great—for example, a distributed version of SSI is possible. However, there remains the problem that for 2PC to successfully commit a transaction, <em>all</em> participants must respond.  Consequently, if <em>any</em> part of the system is broken, the transaction also fails. Distributed transactions thus have a tendency of amplifying failures, which runs counter to our goal of building fault-tolerant systems.</p>
</li>
</ul>
<h4 id="Fault-Tolerant-Consensus"><a href="#Fault-Tolerant-Consensus" class="headerlink" title="Fault-Tolerant Consensus"></a>Fault-Tolerant Consensus</h4><p>The consensus problem is normally formalized as follows: one or more nodes may <em>propose</em> values, and the consensus algorithm <em>decides</em> on one of those values.</p>
<p>In this formalism, a consensus algorithm must satisfy the following properties:</p>
<ul>
<li><p>Uniform agreement: No two nodes decide differently.</p>
</li>
<li><p>Integrity: No node decides twice.</p>
</li>
<li><p>Validity: If a node decides value v, then v was proposed by some node.</p>
</li>
<li><p>Termination: Every node that does not crash eventually decides some value.</p>
</li>
</ul>
<h5 id="Consensus-algorithms-and-total-order-broadcast"><a href="#Consensus-algorithms-and-total-order-broadcast" class="headerlink" title="Consensus algorithms and total order broadcast"></a>Consensus algorithms and total order broadcast</h5><p>The best-known fault-tolerant consensus algorithms are Viewstamped Replication (VSR), Paxos, Raft, and Zab. </p>
<p>Remember that total order broadcast requires messages to be delivered exactly once, in the same order, to all nodes. If you think about it, this is equivalent to performing several rounds of consensus: in each round, nodes propose the message that they want to send next, and then decide on the next message to be delivered in the total order.</p>
<p>So, total order broadcast is equivalent to repeated rounds of consensus (each consen‐ sus decision corresponding to one message delivery):</p>
<ul>
<li><p>Due to the agreement property of consensus, all nodes decide to deliver the same messages in the same order.</p>
</li>
<li><p>Due to the integrity property, messages are not duplicated.</p>
</li>
<li><p>Due to the validity property, messages are not corrupted and not fabricated out of thin air.</p>
</li>
<li><p>Due to the termination property, messages are not lost.</p>
</li>
</ul>
<p>Viewstamped Replication, Raft, and Zab implement total order broadcast directly, because that is more efficient than doing repeated rounds of one-value-at-a-time consensus. In the case of Paxos, this optimization is known as Multi-Paxos.</p>
<h5 id="Single-leader-replication-and-consensus"><a href="#Single-leader-replication-and-consensus" class="headerlink" title="Single-leader replication and consensus"></a>Single-leader replication and consensus</h5><p>It seems that in order to elect a leader, we first need a leader. In order to solve consensus, we must first solve consensus. How do we break out of this conundrum?</p>
<h5 id="Epoch-numbering-and-quorums"><a href="#Epoch-numbering-and-quorums" class="headerlink" title="Epoch numbering and quorums"></a>Epoch numbering and quorums</h5><p>Thus, we have two rounds of voting: once to choose a leader, and a second time to vote on a leader’s proposal. The key insight is that the quorums for those two votes must overlap: if a vote on a proposal succeeds, at least one of the nodes that voted for it must have also participated in the most recent leader election.</p>
<p>This voting process looks superficially similar to two-phase commit. The biggest dif‐ ferences are that in 2PC the coordinator is not elected, and that fault-tolerant consen‐ sus algorithms only require votes from a majority of nodes, whereas 2PC requires a “yes” vote from <em>every</em> participant. Moreover, consensus algorithms define a recovery process by which nodes can get into a consistent state after a new leader is elected, ensuring that the safety properties are always met. These differences are key to the correctness and fault tolerance of a consensus algorithm.</p>
<h5 id="Limitations-of-consensus"><a href="#Limitations-of-consensus" class="headerlink" title="Limitations of consensus"></a>Limitations of consensus</h5><p>The process by which nodes vote on proposals before they are decided is a kind of synchronous replication. </p>
<p>Consensus systems always require a strict majority to operate.</p>
<p>Most consensus algorithms assume a fixed set of nodes that participate in voting, which means that you can’t just add or remove nodes in the cluster. </p>
<p>Consensus systems generally rely on timeouts to detect failed nodes. In environments with highly variable network delays, especially geographically distributed systems, it often happens that a node falsely believes the leader to have failed due to a transient network issue.</p>
<p>Sometimes, consensus algorithms are particularly sensitive to network problems.</p>
<h4 id="Membership-and-Coordination-Services"><a href="#Membership-and-Coordination-Services" class="headerlink" title="Membership and Coordination Services"></a>Membership and Coordination Services</h4><p>ZooKeeper and etcd are designed to hold small amounts of data that can fit entirely in memory (although they still write to disk for durability)—so you wouldn’t want to store all of your application’s data here. That small amount of data is replicated across all the nodes using a fault-tolerant total order broadcast algorithm. </p>
<p>ZooKeeper is modeled after Google’s Chubby lock service [14, 98], implementing not only total order broadcast (and hence consensus), but also an interesting set of other features that turn out to be particularly useful when building distributed systems:</p>
<ul>
<li><p>Linearizable atomic operations</p>
</li>
<li><p>Total ordering of operations</p>
</li>
<li><p>Failure detection</p>
</li>
<li><p>Change notifications</p>
</li>
</ul>
<h5 id="Allocating-work-to-nodes"><a href="#Allocating-work-to-nodes" class="headerlink" title="Allocating work to nodes"></a>Allocating work to nodes</h5><h5 id="Service-discovery"><a href="#Service-discovery" class="headerlink" title="Service discovery"></a>Service discovery</h5><h5 id="Membership-services"><a href="#Membership-services" class="headerlink" title="Membership services"></a>Membership services</h5><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>…</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://rezelchen.github.io/2020/08/13/DDIA8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/images/avatar.gif">
      <meta itemprop="name" content="Ray Chen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ray Chen's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/2020/08/13/DDIA8/" class="post-title-link" itemprop="url">Note for DDIA in Chapter 8</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-08-13 19:53:00" itemprop="dateCreated datePublished" datetime="2020-08-13T19:53:00+00:00">2020-08-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-08-20 07:33:40" itemprop="dateModified" datetime="2020-08-20T07:33:40+00:00">2020-08-20</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="The-Trouble-with-Distributed-Systems"><a href="#The-Trouble-with-Distributed-Systems" class="headerlink" title="The Trouble with Distributed Systems"></a>The Trouble with Distributed Systems</h2><h3 id="Faults-and-Partial-Failures"><a href="#Faults-and-Partial-Failures" class="headerlink" title="Faults and Partial Failures"></a>Faults and Partial Failures</h3><p>An individual computer with good software is usually either fully functional or entirely broken, but not something in between.</p>
<p>This is a deliberate choice in the design of computers: if an internal fault occurs, we prefer a computer to crash completely rather than returning a wrong result, because wrong results are difficult and confusing to deal with. </p>
<p>When you are writing software that runs on several computers, connected by a net‐ work, the situation is fundamentally different. </p>
<p>This nondeterminism and possibility of partial failures is what makes distributed sys‐ tems hard to work with.</p>
<h4 id="Cloud-Computing-and-Supercomputing"><a href="#Cloud-Computing-and-Supercomputing" class="headerlink" title="Cloud Computing and Supercomputing"></a>Cloud Computing and Supercomputing</h4><p>If we want to make distributed systems work, we must accept the possibility of partial failure and build fault-tolerance mechanisms into the software. In other words, we need to build a reliable system from unreliable components.</p>
<h3 id="Unreliable-Networks"><a href="#Unreliable-Networks" class="headerlink" title="Unreliable Networks"></a>Unreliable Networks</h3><h4 id="Network-Faults-in-Practice"><a href="#Network-Faults-in-Practice" class="headerlink" title="Network Faults in Practice"></a>Network Faults in Practice</h4><h4 id="Detecting-Faults"><a href="#Detecting-Faults" class="headerlink" title="Detecting Faults"></a>Detecting Faults</h4><h4 id="Timeouts-and-Unbounded-Delays"><a href="#Timeouts-and-Unbounded-Delays" class="headerlink" title="Timeouts and Unbounded Delays"></a>Timeouts and Unbounded Delays</h4><h5 id="Network-congestion-and-queueing"><a href="#Network-congestion-and-queueing" class="headerlink" title="Network congestion and queueing"></a>Network congestion and queueing</h5><h4 id="Synchronous-Versus-Asynchronous-Networks"><a href="#Synchronous-Versus-Asynchronous-Networks" class="headerlink" title="Synchronous Versus Asynchronous Networks"></a>Synchronous Versus Asynchronous Networks</h4><h5 id="Can-we-not-simply-make-network-delays-predictable"><a href="#Can-we-not-simply-make-network-delays-predictable" class="headerlink" title="Can we not simply make network delays predictable?"></a>Can we not simply make network delays predictable?</h5><h3 id="Unreliable-Clocks"><a href="#Unreliable-Clocks" class="headerlink" title="Unreliable Clocks"></a>Unreliable Clocks</h3><p>Clocks and time are important. Applications depend on clocks in various ways to answer questions like the following:</p>
<ol>
<li>Has this request timed out yet?</li>
<li>What’s the 99th percentile response time of this service?</li>
<li>How many queries per second did this service handle on average in the last five minutes?</li>
<li>How long did the user spend on our site?</li>
<li>When was this article published?</li>
<li>At what date and time should the reminder email be sent?</li>
<li>When does this cache entry expire?</li>
<li>What is the timestamp on this error message in the log file?</li>
</ol>
<h4 id="Monotonic-Versus-Time-of-Day-Clocks"><a href="#Monotonic-Versus-Time-of-Day-Clocks" class="headerlink" title="Monotonic Versus Time-of-Day Clocks"></a>Monotonic Versus Time-of-Day Clocks</h4><h5 id="Time-of-day-clocks"><a href="#Time-of-day-clocks" class="headerlink" title="Time-of-day clocks"></a>Time-of-day clocks</h5><p>A time-of-day clock does what you intuitively expect of a clock: it returns the current date and time according to some calendar (also known as wall-clock time). </p>
<p>Time-of-day clocks are usually synchronized with NTP, which means that a time‐ stamp from one machine (ideally) means the same as a timestamp on another machine. </p>
<h5 id="Monotonic-clocks"><a href="#Monotonic-clocks" class="headerlink" title="Monotonic clocks"></a>Monotonic clocks</h5><p>A monotonic clock is suitable for measuring a duration. The name comes from the fact that they are guaranteed to always move forward (whereas a time-of- day clock may jump back in time).</p>
<p>In a distributed system, using a monotonic clock for measuring elapsed time (e.g., timeouts) is usually fine, because it doesn’t assume any synchronization between dif‐ ferent nodes’ clocks and is not sensitive to slight inaccuracies of measurement.</p>
<h4 id="Clock-Synchronization-and-Accuracy"><a href="#Clock-Synchronization-and-Accuracy" class="headerlink" title="Clock Synchronization and Accuracy"></a>Clock Synchronization and Accuracy</h4><p>It is possible to achieve very good clock accuracy if you care about it sufficiently to invest significant resources. Such accuracy can be achieved using GPS receivers, the Precision Time Protocol (PTP), and careful deployment and monitoring. However, it requires significant effort and expertise, and there are plenty of ways clock synchronization can go wrong.</p>
<h4 id="Relying-on-Synchronized-Clocks"><a href="#Relying-on-Synchronized-Clocks" class="headerlink" title="Relying on Synchronized Clocks"></a>Relying on Synchronized Clocks</h4><p>…</p>
<h4 id="Process-Pauses"><a href="#Process-Pauses" class="headerlink" title="Process Pauses"></a>Process Pauses</h4><h5 id="Response-time-guarantees"><a href="#Response-time-guarantees" class="headerlink" title="Response time guarantees"></a>Response time guarantees</h5><h5 id="Limiting-the-impact-of-garbage-collection"><a href="#Limiting-the-impact-of-garbage-collection" class="headerlink" title="Limiting the impact of garbage collection"></a>Limiting the impact of garbage collection</h5><p>A variant of this idea is to use the garbage collector only for short-lived objects (which are fast to collect) and to restart processes periodically, before they accumu‐ late enough long-lived objects to require a full GC of long-lived objects. One node can be restarted at a time, and traffic can be shifted away from the node before the planned restart, like in a rolling upgrade.</p>
<h3 id="Knowledge-Truth-and-Lies"><a href="#Knowledge-Truth-and-Lies" class="headerlink" title="Knowledge, Truth, and Lies"></a>Knowledge, Truth, and Lies</h3><h4 id="The-Truth-Is-Defined-by-the-Majority"><a href="#The-Truth-Is-Defined-by-the-Majority" class="headerlink" title="The Truth Is Defined by the Majority"></a>The Truth Is Defined by the Majority</h4><p>Instead, many distributed algorithms rely on a quorum, that is, voting among the nodes: decisions require some minimum number of votes from several nodes in order to reduce the dependence on any one particular node.</p>
<h5 id="The-leader-and-the-lock"><a href="#The-leader-and-the-lock" class="headerlink" title="The leader and the lock"></a>The leader and the lock</h5><p>Frequently, a system requires there to be only one of some thing. For example:</p>
<ul>
<li><p>Only one node is allowed to be the leader for a database partition, to avoid split<br>brain.</p>
</li>
<li><p>Only one transaction or client is allowed to hold the lock for a particular resource<br>or object, to prevent concurrently writing to it and corrupting it.</p>
</li>
<li><p>Only one user is allowed to register a particular username, because a username must uniquely identify a user.</p>
</li>
</ul>
<h5 id="Fencing-tokens"><a href="#Fencing-tokens" class="headerlink" title="Fencing tokens"></a>Fencing tokens</h5><p>Checking a token on the server side may seem like a downside, but it is arguably a good thing: it is unwise for a service to assume that its clients will always be well behaved, because the clients are often run by people whose priorities are very differ‐ ent from the priorities of the people running the service. Thus, it is a good idea for any service to protect itself from accidentally abusive clients.</p>
<h4 id="Byzantine-Faults"><a href="#Byzantine-Faults" class="headerlink" title="Byzantine Faults"></a>Byzantine Faults</h4><p>…</p>
<h4 id="System-Model-and-Reality"><a href="#System-Model-and-Reality" class="headerlink" title="System Model and Reality"></a>System Model and Reality</h4><p>With regard to timing assumptions, three system models are in common use:</p>
<ul>
<li><p>Synchronous model: The synchronous model assumes bounded network delay, bounded process pau‐ ses, and bounded clock error. This does not imply exactly synchronized clocks or zero network delay; it just means you know that network delay, pauses, and clock drift will never exceed some fixed upper bound. </p>
</li>
<li><p>Partially synchronous model: Partial synchrony means that a system behaves like a synchronous system most of the time, but it sometimes exceeds the bounds for network delay, process pauses, and clock drift.</p>
</li>
<li><p>Asynchronous model: In this model, an algorithm is not allowed to make any timing assumptions.</p>
</li>
</ul>
<p>Moreover, besides timing issues, we have to consider node failures. The three most common system models for nodes are:</p>
<ul>
<li><p>Crash-stop faults: In the crash-stop model, an algorithm may assume that a node can fail in only one way, namely by crashing. This means that the node may suddenly stop responding at any moment, and thereafter that node is gone forever—it never comes back.</p>
</li>
<li><p>Crash-recovery faults: We assume that nodes may crash at any moment, and perhaps start responding again after some unknown time. In the crash-recovery model, nodes are assumed to have stable storage (i.e., nonvolatile disk storage) that is preserved across crashes, while the in-memory state is assumed to be lost.</p>
</li>
<li><p>Byzantine (arbitrary) faults: Nodes may do absolutely anything, including trying to trick and deceive other nodes, as described in the last section.</p>
</li>
</ul>
<p>For modeling real systems, the partially synchronous model with crash-recovery faults is generally the most useful model. </p>
<h5 id="Correctness-of-an-algorithm"><a href="#Correctness-of-an-algorithm" class="headerlink" title="Correctness of an algorithm"></a>Correctness of an algorithm</h5><h5 id="Safety-and-liveness"><a href="#Safety-and-liveness" class="headerlink" title="Safety and liveness"></a>Safety and liveness</h5><p><em>Safety</em> is often informally defined as <em>nothing bad happens</em>, and <em>liveness</em> as <em>something good eventually happens</em>.</p>
<p>The actual definitions of safety and liveness are precise and mathematical:</p>
<ul>
<li><p>If a safety property is violated, we can point at a particular point in time at which it was broken. After a safety property has been violated, the violation cannot be undone—the damage is already done.</p>
</li>
<li><p>A liveness property works the other way round: it may not hold at some point in time, but there is always hope that it may be satisfied in the future.</p>
</li>
</ul>
<h5 id="Mapping-system-models-to-the-real-world"><a href="#Mapping-system-models-to-the-real-world" class="headerlink" title="Mapping system models to the real world"></a>Mapping system models to the real world</h5><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>…</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://rezelchen.github.io/2020/08/13/DDIA7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/images/avatar.gif">
      <meta itemprop="name" content="Ray Chen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ray Chen's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/2020/08/13/DDIA7/" class="post-title-link" itemprop="url">Note for DDIA in Chapter 7</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-08-13 08:37:00" itemprop="dateCreated datePublished" datetime="2020-08-13T08:37:00+00:00">2020-08-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-08-20 07:33:40" itemprop="dateModified" datetime="2020-08-20T07:33:40+00:00">2020-08-20</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Transactions"><a href="#Transactions" class="headerlink" title="Transactions"></a>Transactions</h2><p>A transaction is a way for an application to group several reads and writes together into a logical unit. Conceptually, all the reads and writes in a transaction are executed as one operation: either the entire transaction succeeds (<em>commit</em>) or it fails (<em>abort, rollback</em>). </p>
<p>In this chapter, we will examine many examples of things that can go wrong, and explore the algorithms that databases use to guard against those issues. We will go especially deep in the area of concurrency control, discussing various kinds of race conditions that can occur and how databases implement isolation levels such as <em>read committed</em>, <em>snapshot isolation</em>, and <em>serializability</em>.</p>
<h3 id="The-Slippery-Concept-of-a-Transaction"><a href="#The-Slippery-Concept-of-a-Transaction" class="headerlink" title="The Slippery Concept of a Transaction"></a>The Slippery Concept of a Transaction</h3><h4 id="The-Meaning-of-ACID"><a href="#The-Meaning-of-ACID" class="headerlink" title="The Meaning of ACID"></a>The Meaning of ACID</h4><h5 id="Atomicity"><a href="#Atomicity" class="headerlink" title="Atomicity"></a>Atomicity</h5><p>If the writes are grouped together into an atomic transaction, and the transaction cannot be completed (committed) due to a fault, then the transaction is aborted and the database must discard or undo any writes it has made so far in that transaction.</p>
<h5 id="Consistency"><a href="#Consistency" class="headerlink" title="Consistency"></a>Consistency</h5><p>The idea of ACID consistency is that you have certain statements about your data (invariants) that must always be true.</p>
<p>However, this idea of consistency depends on the application’s notion of invariants, and it’s the application’s responsibility to define its transactions correctly so that they preserve consistency. </p>
<p>The application may rely on the database’s atomicity and isolation properties in order to achieve consistency, but it’s not up to the database alone. </p>
<h5 id="Isolation"><a href="#Isolation" class="headerlink" title="Isolation"></a>Isolation</h5><p><em>Isolation</em> in the sense of ACID means that concurrently executing transactions are isolated from each other: they cannot step on each other’s toes. The classic database textbooks formalize isolation as <em>serializability</em>, which means that each transaction can pretend that it is the only transaction running on the entire database.</p>
<p>However, in practice, serializable isolation is rarely used, because it carries a performance penalty. </p>
<h5 id="Durability"><a href="#Durability" class="headerlink" title="Durability"></a>Durability</h5><p><em>Durability</em> is the promise that once a transaction has com‐ mitted successfully, any data it has written will not be forgotten, even if there is a hardware fault or the database crashes.</p>
<p>In a replicated database, durabil‐ ity may mean that the data has been successfully copied to some number of nodes. In order to provide a durability guarantee, a database must wait until these writes or replications are complete before reporting a transaction as successfully committed.</p>
<h4 id="Single-Object-and-Multi-Object-Operations"><a href="#Single-Object-and-Multi-Object-Operations" class="headerlink" title="Single-Object and Multi-Object Operations"></a>Single-Object and Multi-Object Operations</h4><h5 id="Single-object-writes"><a href="#Single-object-writes" class="headerlink" title="Single-object writes"></a>Single-object writes</h5><p>Storage engines almost universally aim to provide atomicity and isolation on the level of a single object (such as a key- value pair) on one node. Atomicity can be implemented using a log for crash recovery, and isolation can be implemented using a lock on each object (allowing only one thread to access an object at any one time).</p>
<p>Some databases also provide more complex atomic operations,iv such as an increment operation and compare-and-set operation.</p>
<h5 id="The-need-for-multi-object-transactions"><a href="#The-need-for-multi-object-transactions" class="headerlink" title="The need for multi-object transactions"></a>The need for multi-object transactions</h5><h5 id="Handling-errors-and-aborts"><a href="#Handling-errors-and-aborts" class="headerlink" title="Handling errors and aborts"></a>Handling errors and aborts</h5><p>A key feature of a transaction is that it can be aborted and safely retried if an error occurred. </p>
<p>Although retrying an aborted transaction is a simple and effective error handling mechanism, it isn’t perfect:</p>
<ul>
<li><p>If the transaction actually succeeded, but the network failed while the server tried to acknowledge the successful commit to the client (so the client thinks it failed), then retrying the transaction causes it to be performed twice—unless you have an additional application-level deduplication mechanism in place.</p>
</li>
<li><p>If the error is due to overload, retrying the transaction will make the problem worse, not better. To avoid such feedback cycles, you can limit the number of retries, use exponential backoff, and handle overload-related errors differently from other errors (if possible).</p>
</li>
<li><p>It is only worth retrying after transient errors (for example due to deadlock, iso‐ lation violation, temporary network interruptions, and failover); after a perma‐ nent error (e.g., constraint violation) a retry would be pointless.</p>
</li>
<li><p>If the transaction also has side effects outside of the database, those side effects may happen even if the transaction is aborted. For example, if you’re sending an email, you wouldn’t want to send the email again every time you retry the trans‐ action. If you want to make sure that several different systems either commit or abort together, two-phase commit can help.</p>
</li>
<li><p>If the client process fails while retrying, any data it was trying to write to the database is lost.</p>
</li>
</ul>
<h3 id="Weak-Isolation-Levels"><a href="#Weak-Isolation-Levels" class="headerlink" title="Weak Isolation Levels"></a>Weak Isolation Levels</h3><h4 id="Read-Committed"><a href="#Read-Committed" class="headerlink" title="Read Committed"></a>Read Committed</h4><p>The most basic level of transaction isolation is read committed.v It makes two guarantees:</p>
<ol>
<li><p>When reading from the database, you will only see data that has been committed (no dirty reads).</p>
</li>
<li><p>When writing to the database, you will only overwrite data that has been committed (no dirty writes).</p>
</li>
</ol>
<h5 id="No-dirty-reads"><a href="#No-dirty-reads" class="headerlink" title="No dirty reads"></a>No dirty reads</h5><p>There are a few reasons why it’s useful to prevent dirty reads:</p>
<ul>
<li><p>If a transaction needs to update several objects, a dirty read means that another transaction may see some of the updates but not others.</p>
</li>
<li><p>If a transaction aborts, any writes it has made need to be rolled back. If the database allows dirty reads, that means a transaction may see data that is later rolled back.</p>
</li>
</ul>
<h5 id="No-dirty-writes"><a href="#No-dirty-writes" class="headerlink" title="No dirty writes"></a>No dirty writes</h5><p>Transactions running at the read committed isolation level must prevent dirty writes, usually by delaying the second write until the first write’s transaction has committed or aborted.</p>
<p>By preventing dirty writes, this isolation level avoids some kinds of concurrency problems:</p>
<ul>
<li><p>If transactions update multiple objects, dirty writes can lead to a bad outcome.</p>
</li>
<li><p>However, read committed does not prevent the race condition between two counter increments in Figure 7-1.</p>
</li>
</ul>
<h5 id="Implementing-read-committed"><a href="#Implementing-read-committed" class="headerlink" title="Implementing read committed"></a>Implementing read committed</h5><p>Most commonly, databases prevent dirty writes by using row-level locks: when a transaction wants to modify a particular object (row or document), it must first acquire a lock on that object. It must then hold that lock until the transaction is committed or aborted.</p>
<p>How do we prevent dirty reads? One option would be to use the same lock, and to require any transaction that wants to read an object to briefly acquire the lock and then release it again immediately after reading.</p>
<p>However, the approach of requiring read locks does not work well in practice, because one long-running write transaction can force many read-only transactions to wait until the long-running transaction has completed.</p>
<p>The database remembers both the old committed value and the new value set by the transaction that currently holds the write lock. While the transaction is ongoing, any other transactions that read the object are simply given the old value. Only when the new value is committed do transactions switch over to reading the new value.</p>
<h4 id="Snapshot-Isolation-and-Repeatable-Read"><a href="#Snapshot-Isolation-and-Repeatable-Read" class="headerlink" title="Snapshot Isolation and Repeatable Read"></a>Snapshot Isolation and Repeatable Read</h4><p><em>nonrepeatable read</em> or <em>read skew</em></p>
<p>Some situations cannot tolerate such temporary inconsistency:</p>
<ul>
<li>Backups</li>
<li>Analytic queries and integrity checks</li>
</ul>
<p><em>Snapshot isolation</em> is the most common solution to this problem. The idea is that each transaction reads from a consistent snapshot of the database—that is, the trans‐ action sees all the data that was committed in the database at the start of the transac‐ tion. Even if the data is subsequently changed by another transaction, each transaction sees only the old data from that particular point in time.</p>
<p>Snapshot isolation is a boon for long-running, read-only queries such as backups and analytics.</p>
<h5 id="Implementing-snapshot-isolation"><a href="#Implementing-snapshot-isolation" class="headerlink" title="Implementing snapshot isolation"></a>Implementing snapshot isolation</h5><p>Like read committed isolation, implementations of snapshot isolation typically use write locks to prevent dirty writes. However, reads do not require any locks. From a performance point of view, a key principle of snapshot isolation is <em>readers never block writers, and writers never block readers</em>. This allows a database to handle long-running read queries on a consistent snapshot at the same time as processing writes normally, without any lock contention between the two.</p>
<p>The database must potentially keep several different committed versions of an object, because various in-progress trans‐ actions may need to see the state of the database at different points in time. Because it maintains several versions of an object side by side, this technique is known as <em>multi- version concurrency control</em> (MVCC).</p>
<p>An update is internally translated into a delete and a create. </p>
<h5 id="Visibility-rules-for-observing-a-consistent-snapshot"><a href="#Visibility-rules-for-observing-a-consistent-snapshot" class="headerlink" title="Visibility rules for observing a consistent snapshot"></a>Visibility rules for observing a consistent snapshot</h5><p>When a transaction reads from the database, transaction IDs are used to decide which objects it can see and which are invisible. By carefully defining visibility rules, the database can present a consistent snapshot of the database to the application. This works as follows:</p>
<ol>
<li><p>At the start of each transaction, the database makes a list of all the other transactions that are in progress (not yet committed or aborted) at that time. Any writes that those transactions have made are ignored, even if the transactions subsequently commit.</p>
</li>
<li><p>Any writes made by aborted transactions are ignored.</p>
</li>
<li><p>Any writes made by transactions with a later transaction ID (i.e., which started after the current transaction started) are ignored, regardless of whether those transactions have committed.</p>
</li>
<li><p>All other writes are visible to the application’s queries.</p>
</li>
</ol>
<p>Put another way, an object is visible if both of the following conditions are true:</p>
<ul>
<li><p>At the time when the reader’s transaction started, the transaction that created the object had already committed.</p>
</li>
<li><p>The object is not marked for deletion, or if it is, the transaction that requested deletion had not yet committed at the time when the reader’s transaction started.</p>
</li>
</ul>
<h5 id="Indexes-and-snapshot-isolation"><a href="#Indexes-and-snapshot-isolation" class="headerlink" title="Indexes and snapshot isolation"></a>Indexes and snapshot isolation</h5><p>…</p>
<h5 id="Repeatable-read-and-naming-confusion"><a href="#Repeatable-read-and-naming-confusion" class="headerlink" title="Repeatable read and naming confusion"></a>Repeatable read and naming confusion</h5><h4 id="Preventing-Lost-Updates"><a href="#Preventing-Lost-Updates" class="headerlink" title="Preventing Lost Updates"></a>Preventing Lost Updates</h4><p>The <em>lost update</em> problem can occur if an application reads some value from the data‐ base, modifies it, and writes back the modified value (a read-modify-write cycle). If two transactions do this concurrently, one of the modifications can be lost, because the second write does not include the first modification. (We sometimes say that the later write <em>clobbers</em> the earlier write.) This pattern occurs in various different scenarios:</p>
<ul>
<li><p>Incrementing a counter or updating an account balance (requires reading the current value, calculating the new value, and writing back the updated value)</p>
</li>
<li><p>Making a local change to a complex value, e.g., adding an element to a list within a JSON document (requires parsing the document, making the change, and writ‐ ing back the modified document)</p>
</li>
<li><p>Two users editing a wiki page at the same time, where each user saves their changes by sending the entire page contents to the server, overwriting whatever is currently in the database</p>
</li>
</ul>
<h5 id="Atomic-write-operations"><a href="#Atomic-write-operations" class="headerlink" title="Atomic write operations"></a>Atomic write operations</h5><p>Many databases provide atomic update operations, which remove the need to implement read-modify-write cycles in application code. They are usually the best solution if your code can be expressed in terms of those operations. </p>
<p>Atomic operations are usually implemented by taking an exclusive lock on the object when it is read so that no other transaction can read it until the update has been applied. This technique is sometimes known as cursor stability. Another option is to simply force all atomic operations to be executed on a single thread.</p>
<h5 id="Explicit-locking"><a href="#Explicit-locking" class="headerlink" title="Explicit locking"></a>Explicit locking</h5><p>Another option for preventing lost updates, if the database’s built-in atomic opera‐ tions don’t provide the necessary functionality, is for the application to explicitly lock objects that are going to be updated. Then the application can perform a read- modify-write cycle, and if any other transaction tries to concurrently read the same object, it is forced to wait until the first read-modify-write cycle has completed.</p>
<h5 id="Automatically-detecting-lost-updates"><a href="#Automatically-detecting-lost-updates" class="headerlink" title="Automatically detecting lost updates"></a>Automatically detecting lost updates</h5><p>An alternative is to allow them to execute in parallel and, if the transaction manager detects a lost update, abort the transaction and force it to retry its read-modify-write cycle.</p>
<p>An advantage of this approach is that databases can perform this check efficiently in conjunction with snapshot isolation.</p>
<h5 id="Compare-and-set"><a href="#Compare-and-set" class="headerlink" title="Compare-and-set"></a>Compare-and-set</h5><p>The purpose of this operation is to avoid lost updates by allowing an update to happen only if the value has not changed since you last read it. If the current value does not match what you previously read, the update has no effect, and the read-modify-write cycle must be retried.</p>
<p>Check whether your database’s compare-and-set operation is safe before relying on it.</p>
<h5 id="Conflict-resolution-and-replication"><a href="#Conflict-resolution-and-replication" class="headerlink" title="Conflict resolution and replication"></a>Conflict resolution and replication</h5><p>Locks and compare-and-set operations assume that there is a single up-to-date copy of the data. However, they do not apply in the context that databases with multi-leader or leaderless replication.</p>
<p>Atomic operations can work well in a replicated context.</p>
<p>On the other hand, the last write wins (LWW) conflict resolution method is prone to lost updates.</p>
<h4 id="Write-Skew-and-Phantoms"><a href="#Write-Skew-and-Phantoms" class="headerlink" title="Write Skew and Phantoms"></a>Write Skew and Phantoms</h4><h5 id="Characterizing-write-skew"><a href="#Characterizing-write-skew" class="headerlink" title="Characterizing write skew"></a>Characterizing write skew</h5><p><em>Write skew</em> is neither a dirty write nor a lost update, because the two transactions are updating two different objects.</p>
<p>You can think of write skew as a generalization of the lost update problem. Write skew can occur if two transactions read the same objects, and then update some of those objects (different transactions may update different objects). In the special case where different transactions update the same object, you get a dirty write or lost update anomaly (depending on the timing).</p>
<p>If you can’t use a serializable isolation level, the second-best option in this case is probably to explicitly lock the rows that the transaction depends on. </p>
<h5 id="More-examples-of-write-skew"><a href="#More-examples-of-write-skew" class="headerlink" title="More examples of write skew"></a>More examples of write skew</h5><h5 id="Phantoms-causing-write-skew"><a href="#Phantoms-causing-write-skew" class="headerlink" title="Phantoms causing write skew"></a>Phantoms causing write skew</h5><p>All of these examples follow a similar pattern:</p>
<ol>
<li><p>A SELECT query checks whether some requirement is satisfied by searching for rows that match some search condition (there are at least two doctors on call, there are no existing bookings for that room at that time, the position on the board doesn’t already have another figure on it, the username isn’t already taken, there is still money in the account).</p>
</li>
<li><p>Depending on the result of the first query, the application code decides how to continue (perhaps to go ahead with the operation, or perhaps to report an error to the user and abort).</p>
</li>
<li><p>If the application decides to go ahead, it makes a write (INSERT, UPDATE, or DELETE) to the database and commits the transaction.</p>
</li>
<li><p>The effect of this write changes the precondition of the decision of step 2. In other words, if you were to repeat the SELECT query from step 1 after commiting the write, you would get a different result, because the write changed the set of rows matching the search condition (there is now one fewer doctor on call, the meeting room is now booked for that time, the position on the board is now taken by the figure that was moved, the username is now taken, there is now less money in the account).</p>
</li>
</ol>
<p>This effect, where a write in one transaction changes the result of a search query in another transaction, is called a <em>phantom</em>. Snapshot isolation avoids phantoms in read-only queries, but in read-write transactions like the examples we discussed, phantoms can lead to particularly tricky cases of write skew.</p>
<h5 id="Materializing-conflicts"><a href="#Materializing-conflicts" class="headerlink" title="Materializing conflicts"></a>Materializing conflicts</h5><p>If the problem of phantoms is that there is no object to which we can attach the locks, perhaps we can artificially introduce a lock object into the database?</p>
<p>This approach is called <em>materializing conflicts</em>, because it takes a phantom and turns it into a lock conflict on a concrete set of rows that exist in the database.</p>
<h3 id="Serializability"><a href="#Serializability" class="headerlink" title="Serializability"></a>Serializability</h3><p>Most databases that provide serializability today use one of three techniques, which we will explore in the rest of this chapter:</p>
<ul>
<li><p>Literally executing transactions in a serial order</p>
</li>
<li><p>Two-phase locking, which for several decades was the only viable option</p>
</li>
<li><p>Optimistic concurrency control techniques such as serializable snapshot isolation</p>
</li>
</ul>
<h4 id="Actual-Serial-Execution"><a href="#Actual-Serial-Execution" class="headerlink" title="Actual Serial Execution"></a>Actual Serial Execution</h4><p>Two developments caused this rethink:</p>
<ul>
<li><p>RAM became cheap enough that for many use cases is now feasible to keep the entire active dataset in memory.</p>
</li>
<li><p>Database designers realized that OLTP transactions are usually short and only make a small number of reads and writes. By contrast, long-running analytic queries are typically read- only, so they can be run on a consistent snapshot (using snapshot isolation) outside of the serial execution loop.</p>
</li>
</ul>
<p>In order to make the most of that single thread, transac‐ tions need to be structured differently from their traditional form.</p>
<h5 id="Encapsulating-transactions-in-stored-procedures"><a href="#Encapsulating-transactions-in-stored-procedures" class="headerlink" title="Encapsulating transactions in stored procedures"></a>Encapsulating transactions in stored procedures</h5><p>The application must submit the entire transaction code to the database ahead of time, as a <em>stored procedure</em>. </p>
<h5 id="Pros-and-cons-of-stored-procedures"><a href="#Pros-and-cons-of-stored-procedures" class="headerlink" title="Pros and cons of stored procedures"></a>Pros and cons of stored procedures</h5><p>Stored procedures have gained a somewhat bad reputation, for various reasons:</p>
<ul>
<li><p>Each database vendor has its own language for stored procedures. These languages haven’t kept up with developments in general-purpose programming languages, so they look quite ugly and archaic from today’s point of view, and they lack the ecosystem of libraries that you find with most programming languages.</p>
</li>
<li><p>Code running in a database is difficult to manage: compared to an application server, it’s harder to debug, more awkward to keep in version control and deploy, trickier to test, and difficult to integrate with a metrics collection system for monitoring.</p>
</li>
<li><p>A database is often much more performance-sensitive than an application server. A badly written stored procedure in a database can cause much more trouble than equivalent badly written code in an application server.</p>
</li>
</ul>
<p>With stored procedures and in-memory data, executing all transactions on a single thread becomes feasible. </p>
<h5 id="Partitioning"><a href="#Partitioning" class="headerlink" title="Partitioning"></a>Partitioning</h5><p>If you can find a way of partitioning your dataset so that each transaction only needs to read and write data within a single partition, then each partition can have its own transaction processing thread running independently from the others. </p>
<p>However, for any transaction that needs to access multiple partitions, the database must coordinate the transaction across all the partitions that it touches. The stored procedure needs to be performed in lock-step across all partitions to ensure serializa‐ bility across the whole system.</p>
<h5 id="Summary-of-serial-execution"><a href="#Summary-of-serial-execution" class="headerlink" title="Summary of serial execution"></a>Summary of serial execution</h5><p>Serial execution of transactions has become a viable way of achieving serializable iso‐ lation within certain constraints:</p>
<ul>
<li><p>Every transaction must be small and fast, because it takes only one slow transac‐ tion to stall all transaction processing.</p>
</li>
<li><p>It is limited to use cases where the active dataset can fit in memory. Rarely accessed data could potentially be moved to disk, but if it needed to be accessed in a single-threaded transaction, the system would get very slow.x</p>
</li>
<li><p>Write throughput must be low enough to be handled on a single CPU core, or else transactions need to be partitioned without requiring cross-partition coordination.</p>
</li>
<li><p>Cross-partition transactions are possible, but there is a hard limit to the extent to which they can be used.</p>
</li>
</ul>
<h4 id="Two-Phase-Locking-2PL"><a href="#Two-Phase-Locking-2PL" class="headerlink" title="Two-Phase Locking (2PL)"></a>Two-Phase Locking (2PL)</h4><p>Sev‐ eral transactions are allowed to concurrently read the same object as long as nobody is writing to it. But as soon as anyone wants to write (modify or delete) an object, exclusive access is required:</p>
<ul>
<li><p>If transaction A has read an object and transaction B wants to write to that object, B must wait until A commits or aborts before it can continue. (This ensures that B can’t change the object unexpectedly behind A’s back.)</p>
</li>
<li><p>If transaction A has written an object and transaction B wants to read that object, B must wait until A commits or aborts before it can continue. </p>
</li>
</ul>
<p>In 2PL, writers don’t just block other writers; they also block readers and vice versa.</p>
<h5 id="Implementation-of-two-phase-locking"><a href="#Implementation-of-two-phase-locking" class="headerlink" title="Implementation of two-phase locking"></a>Implementation of two-phase locking</h5><p>The blocking of readers and writers is implemented by a having a lock on each object in the database. The lock can either be in <em>shared mode</em> or in <em>exclusive mode</em>. The lock is used as follows:</p>
<ul>
<li><p>If a transaction wants to read an object, it must first acquire the lock in shared mode. Several transactions are allowed to hold the lock in shared mode simulta‐ neously, but if another transaction already has an exclusive lock on the object, these transactions must wait.</p>
</li>
<li><p>If a transaction wants to write to an object, it must first acquire the lock in exclusive mode. </p>
</li>
<li><p>If a transaction first reads and then writes an object, it may upgrade its shared lock to an exclusive lock. The upgrade works the same as getting an exclusive lock directly.</p>
</li>
<li><p>After a transaction has acquired the lock, it must continue to hold the lock until the end of the transaction (commit or abort). This is where the name “two-phase” comes from: the first phase (while the transaction is executing) is when the locks are acquired, and the second phase (at the end of the transaction) is when all the locks are released.</p>
</li>
</ul>
<p>Since so many locks are in use, it can happen quite easily that transaction A is stuck waiting for transaction B to release its lock, and vice versa. This situation is called deadlock. The database automatically detects deadlocks between transactions and aborts one of them so that the others can make progress. The aborted transaction needs to be retried by the application.</p>
<h5 id="Performance-of-two-phase-locking"><a href="#Performance-of-two-phase-locking" class="headerlink" title="Performance of two-phase locking"></a>Performance of two-phase locking</h5><p>Although deadlocks can happen with the lock-based read committed isolation level, they occur much more frequently under 2PL serializable isolation (depending on the access patterns of your transaction). This can be an additional performance problem: when a transaction is aborted due to deadlock and is retried, it needs to do its work all over again. If deadlocks are frequent, this can mean significant wasted effort.</p>
<h5 id="Predicate-locks"><a href="#Predicate-locks" class="headerlink" title="Predicate locks"></a>Predicate locks</h5><p>A database with serializable isolation must prevent phantoms.</p>
<p>How do we implement this? Conceptually, we need a <em>predicate lock</em>. It works similarly to the shared/exclusive lock described earlier, but rather than belonging to a particular object (e.g., one row in a table), it belongs to all objects that match some search condition.</p>
<p>A predicate lock restricts access as follows:</p>
<ul>
<li><p>If transaction A wants to read objects matching some condition, like in that SELECT query, it must acquire a shared-mode predicate lock on the conditions of the query. If another transaction B currently has an exclusive lock on any object matching those conditions, A must wait until B releases its lock before it is allowed to make its query.</p>
</li>
<li><p>If transaction A wants to insert, update, or delete any object, it must first check whether either the old or the new value matches any existing predicate lock. If there is a matching predicate lock held by transaction B, then A must wait until B has committed or aborted before it can continue.</p>
</li>
</ul>
<p>The key idea here is that a predicate lock applies even to objects that do not yet exist in the database, but which might be added in the future (phantoms). If two-phase locking includes predicate locks, the database prevents all forms of write skew and other race conditions, and so its isolation becomes serializable.</p>
<h5 id="Index-range-locks"><a href="#Index-range-locks" class="headerlink" title="Index-range locks"></a>Index-range locks</h5><p>Most databases with 2PL actually implement <em>index-range locking</em> (also known as <em>next-key locking</em>), which is a simplified approximation of predicate locking.</p>
<p>Index-range locks are not as precise as predicate locks would be (they may lock a bigger range of objects than is strictly necessary to maintain serializability), but since they have much lower overheads, they are a good compromise.</p>
<p>If there is no suitable index where a range lock can be attached, the database can fall back to a shared lock on the entire table.</p>
<h4 id="Serializable-Snapshot-Isolation-SSI"><a href="#Serializable-Snapshot-Isolation-SSI" class="headerlink" title="Serializable Snapshot Isolation (SSI)"></a>Serializable Snapshot Isolation (SSI)</h4><h5 id="Pessimistic-versus-optimistic-concurrency-control"><a href="#Pessimistic-versus-optimistic-concurrency-control" class="headerlink" title="Pessimistic versus optimistic concurrency control"></a>Pessimistic versus optimistic concurrency control</h5><p>Two-phase locking is a so-called <em>pessimistic</em> concurrency control mechanism.</p>
<p>By contrast, serializable snapshot isolation is an <em>optimistic</em> concurrency control technique. Optimistic in this context means that instead of blocking if something potentially dangerous happens, transactions continue anyway, in the hope that everything will turn out all right. When a transaction wants to commit, the database checks whether anything bad happened (i.e., whether isolation was violated); if so, the transaction is aborted and has to be retried. Only transactions that executed serializably are allowed to commit.</p>
<p>On top of snapshot isolation, SSI adds an algorithm for detecting serialization conflicts among writes and determining which transactions to abort.</p>
<h5 id="Decisions-based-on-an-outdated-premise"><a href="#Decisions-based-on-an-outdated-premise" class="headerlink" title="Decisions based on an outdated premise"></a>Decisions based on an outdated premise</h5><p>To be safe, the database needs to assume that any change in the query result (the premise) means that writes in that transaction may be invalid.  In order to provide serializable isolation, the database must detect situations in which a transaction may have acted on an outdated premise and abort the transaction in that case.</p>
<p>How does the database know if a query result might have changed? There are two cases to consider:</p>
<ul>
<li><p>Detecting reads of a stale MVCC object version (uncommitted write occurred before the read)</p>
</li>
<li><p>Detecting writes that affect prior reads (the write occurs after the read)</p>
</li>
</ul>
<h5 id="Detecting-stale-MVCC-reads"><a href="#Detecting-stale-MVCC-reads" class="headerlink" title="Detecting stale MVCC reads"></a>Detecting stale MVCC reads</h5><p>When the transac‐ tion wants to commit, the database checks whether any of the ignored writes have now been committed. If so, the transaction must be aborted.</p>
<p>By avoiding unnecessary aborts, SSI preserves snapshot isolation’s support for long-running reads from a consistent snapshot.</p>
<h5 id="Detecting-writes-that-affect-prior-reads"><a href="#Detecting-writes-that-affect-prior-reads" class="headerlink" title="Detecting writes that affect prior reads"></a>Detecting writes that affect prior reads</h5><p>When a transaction writes to the database, it must look in the indexes for any other transactions that have recently read the affected data. This process is similar to acquiring a write lock on the affected key range, but rather than blocking until the readers have committed, the lock acts as a tripwire: it simply notifies the transactions that the data they read may no longer be up to date.</p>
<h5 id="Performance-of-serializable-snapshot-isolation"><a href="#Performance-of-serializable-snapshot-isolation" class="headerlink" title="Performance of serializable snapshot isolation"></a>Performance of serializable snapshot isolation</h5><p>As always, many engineering details affect how well an algorithm works in practice. For example, one trade-off is the granularity at which transactions’ reads and writes are tracked. Less detailed tracking is faster, but may lead to more transac‐ tions being aborted than strictly necessary.</p>
<p>In some cases, it’s okay for a transaction to read information that was overwritten by another transaction: depending on what else happened, it’s sometimes possible to prove that the result of the execution is nevertheless serializable. PostgreSQL uses this theory to reduce the number of unnecessary aborts.</p>
<p>Compared to two-phase locking, the big advantage of serializable snapshot isolation is that one transaction doesn’t need to block waiting for locks held by another transaction. </p>
<p>The rate of aborts significantly affects the overall performance of SSI.</p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>…</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://rezelchen.github.io/2020/08/12/DDIA6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/images/avatar.gif">
      <meta itemprop="name" content="Ray Chen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ray Chen's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/2020/08/12/DDIA6/" class="post-title-link" itemprop="url">Note for DDIA in Chapter 6</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-08-12 21:43:00" itemprop="dateCreated datePublished" datetime="2020-08-12T21:43:00+00:00">2020-08-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-08-20 07:33:40" itemprop="dateModified" datetime="2020-08-20T07:33:40+00:00">2020-08-20</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Partitioning"><a href="#Partitioning" class="headerlink" title="Partitioning"></a>Partitioning</h2><p>In this chapter we will first look at different approaches for partitioning large datasets and observe how the indexing of data interacts with partitioning. We’ll then talk about rebalancing, which is necessary if you want to add or remove nodes in your cluster. Finally, we’ll get an overview of how databases route requests to the right par‐ titions and execute queries.</p>
<h3 id="Partitioning-and-Replication"><a href="#Partitioning-and-Replication" class="headerlink" title="Partitioning and Replication"></a>Partitioning and Replication</h3><h3 id="Partitioning-of-Key-Value-Data"><a href="#Partitioning-of-Key-Value-Data" class="headerlink" title="Partitioning of Key-Value Data"></a>Partitioning of Key-Value Data</h3><h4 id="Partitioning-by-Key-Range"><a href="#Partitioning-by-Key-Range" class="headerlink" title="Partitioning by Key Range"></a>Partitioning by Key Range</h4><p>This partitioning strategy is used by Bigtable, its open source equivalent HBase, RethinkDB, and MongoDB before version 2.4.</p>
<p>Within each partition, we can keep keys in sorted order. This has the advantage that range scans are easy, and you can treat the key as a concatenated index in order to fetch several related records in one query.</p>
<p>However, the downside of key range partitioning is that certain access patterns can lead to <em>hot spots</em>.</p>
<h4 id="Partitioning-by-Hash-of-Key"><a href="#Partitioning-by-Hash-of-Key" class="headerlink" title="Partitioning by Hash of Key"></a>Partitioning by Hash of Key</h4><p>A good hash function takes skewed data and makes it uniformly distributed. </p>
<p>For partitioning purposes, the hash function need not be cryptographically strong: for example, Cassandra and MongoDB use MD5, and Voldemort uses the Fowler– Noll–Vo function. </p>
<p>Unfortunately however, by using the hash of the key for partitioning we lose a nice property of key-range partitioning: the ability to do efficient range queries. </p>
<h4 id="Skewed-Workloads-and-Relieving-Hot-Spots"><a href="#Skewed-Workloads-and-Relieving-Hot-Spots" class="headerlink" title="Skewed Workloads and Relieving Hot Spots"></a>Skewed Workloads and Relieving Hot Spots</h4><h3 id="Partitioning-and-Secondary-Indexes"><a href="#Partitioning-and-Secondary-Indexes" class="headerlink" title="Partitioning and Secondary Indexes"></a>Partitioning and Secondary Indexes</h3><p>The problem with secondary indexes is that they don’t map neatly to partitions. There are two main approaches to partitioning a database with secondary indexes: document-based partitioning and term-based partitioning.</p>
<h4 id="Partitioning-Secondary-Indexes-by-Document"><a href="#Partitioning-Secondary-Indexes-by-Document" class="headerlink" title="Partitioning Secondary Indexes by Document"></a>Partitioning Secondary Indexes by Document</h4><p>In this indexing approach, each partition is completely separate: each partition main‐ tains its own secondary indexes, covering only the documents in that partition. For that reason, a document-partitioned index is also known as a local index (as opposed to a global index, described in the next section).</p>
<p>You need to send the query to all partitions, and combine all the results you get back.</p>
<p>This approach to querying a partitioned database is sometimes known as <em>scatter/ gather</em>, and it can make read queries on secondary indexes quite expensive. </p>
<h4 id="Partitioning-Secondary-Indexes-by-Term"><a href="#Partitioning-Secondary-Indexes-by-Term" class="headerlink" title="Partitioning Secondary Indexes by Term"></a>Partitioning Secondary Indexes by Term</h4><p>A global index must also be partitioned, but it can be partitioned differently from the primary key index.</p>
<p>We call this kind of index <em>term-partitioned</em>, because the term we’re looking for determines the partition of the index. </p>
<p>The advantage of a global (term-partitioned) index over a document-partitioned index is that it can make reads more efficient: rather than doing scatter/gather over all partitions, a client only needs to make a request to the partition containing the term that it wants. However, the downside of a global index is that writes are slower and more complicated, because a write to a single document may now affect multiple partitions of the index (every term in the document might be on a different partition, on a different node).</p>
<p>In practice, updates to global secondary indexes are often asynchronous (that is, if you read the index shortly after a write, the change you just made may not yet be reflected in the index).</p>
<h3 id="Rebalancing-Partitions"><a href="#Rebalancing-Partitions" class="headerlink" title="Rebalancing Partitions"></a>Rebalancing Partitions</h3><p>The process of moving load from one node in the cluster to another is called <em>rebalancing</em>.</p>
<p>No matter which partitioning scheme is used, rebalancing is usually expected to meet some minimum requirements:</p>
<ul>
<li><p>After rebalancing, the load (data storage, read and write requests) should be shared fairly between the nodes in the cluster.</p>
</li>
<li><p>While rebalancing is happening, the database should continue accepting reads and writes.</p>
</li>
<li><p>No more data than necessary should be moved between nodes, to make rebalancing fast and to minimize the network and disk I/O load.</p>
</li>
</ul>
<h4 id="Strategies-for-Rebalancing"><a href="#Strategies-for-Rebalancing" class="headerlink" title="Strategies for Rebalancing"></a>Strategies for Rebalancing</h4><h5 id="How-not-to-do-it-hash-mod-N"><a href="#How-not-to-do-it-hash-mod-N" class="headerlink" title="How not to do it: hash mod N"></a>How not to do it: hash mod N</h5><p>When partitioning by the hash of a key, we said earlier that it’s best to divide the possible hashes into ranges and assign each range to a partition (e.g., assign key to partition 0 if 0 ≤ hash(key) &lt; b0, to partition 1 if b0 ≤ hash(key) &lt; b1, etc.).</p>
<p>The problem with the mod N approach is that if the number of nodes N changes, most of the keys will need to be moved from one node to another. </p>
<h5 id="Fixed-number-of-partitions"><a href="#Fixed-number-of-partitions" class="headerlink" title="Fixed number of partitions"></a>Fixed number of partitions</h5><p>Fortunately, there is a fairly simple solution: create many more partitions than there are nodes, and assign several partitions to each node.  For example, a database running on a cluster of 10 nodes may be split into 1,000 partitions from the outset so that approximately 100 partitions are assigned to each node.</p>
<p>Now, if a node is added to the cluster, the new node can <em>steal</em> a few partitions from every existing node until partitions are fairly distributed once again. If a node is removed from the cluster, the same happens in reverse.</p>
<p>In this configuration, the number of partitions is usually fixed when the database is first set up and not changed afterward. </p>
<h5 id="Dynamic-partitioning"><a href="#Dynamic-partitioning" class="headerlink" title="Dynamic partitioning"></a>Dynamic partitioning</h5><p>For databases that use key range partitioning, a fixed number of partitions with fixed boundaries would be very incon‐ venient: if you got the boundaries wrong, you could end up with all of the data in one partition and all of the other partitions empty.</p>
<p>When a partition grows to exceed a configured size (on HBase, the default is 10 GB), it is split into two partitions so that approximately half of the data ends up on each side of the split. Conversely, if lots of data is deleted and a partition shrinks below some threshold, it can be merged with an adjacent par‐ tition. This process is similar to what happens at the top level of a B-tree.</p>
<p>Each partition is assigned to one node, and each node can handle multiple partitions, like in the case of a fixed number of partitions. After a large partition has been split, one of its two halves can be transferred to another node in order to balance the load.</p>
<p>An advantage of dynamic partitioning is that the number of partitions adapts to the total data volume. </p>
<p>However, a caveat is that an empty database starts off with a single partition, since there is no a priori information about where to draw the partition boundaries. While the dataset is small—until it hits the point at which the first partition is split—all writes have to be processed by a single node while the other nodes sit idle. </p>
<p>Dynamic partitioning is not only suitable for key range–partitioned data, but can equally well be used with hash-partitioned data. </p>
<h5 id="Partitioning-proportionally-to-nodes"><a href="#Partitioning-proportionally-to-nodes" class="headerlink" title="Partitioning proportionally to nodes"></a>Partitioning proportionally to nodes</h5><p>With dynamic partitioning, the number of partitions is proportional to the size of the dataset, since the splitting and merging processes keep the size of each partition between some fixed minimum and maximum. On the other hand, with a fixed number of partitions, the size of each partition is proportional to the size of the dataset. In both of these cases, the number of partitions is independent of the number of nodes.</p>
<p>A third option, used by Cassandra and Ketama, is to make the number of partitions proportional to the number of nodes—in other words, to have a fixed number of partitions <em>per node</em>.</p>
<p>When a new node joins the cluster, it randomly chooses a fixed number of existing partitions to split, and then takes ownership of one half of each of those split partitions while leaving the other half of each partition in place. </p>
<p>Picking partition boundaries randomly requires that hash-based partitioning is used. ndeed, this approach corresponds most closely to the original definition of consistent hashing. Newer hash func‐ tions can achieve a similar effect with lower metadata overhead.</p>
<h4 id="Operations-Automatic-or-Manual-Rebalancing"><a href="#Operations-Automatic-or-Manual-Rebalancing" class="headerlink" title="Operations: Automatic or Manual Rebalancing"></a>Operations: Automatic or Manual Rebalancing</h4><h3 id="Request-Routing"><a href="#Request-Routing" class="headerlink" title="Request Routing"></a>Request Routing</h3><p>Somebody needs to stay on top of those changes in order to answer the question: if I want to read or write the key “foo”, which IP address and port number do I need to connect to?</p>
<p>This is an instance of a more general problem called <em>service discovery</em>, which isn’t limited to just databases. </p>
<p>On a high level, there are a few different approaches to this problem:</p>
<ol>
<li><p>Allow clients to contact any node (e.g., via a round-robin load balancer). If that node coincidentally owns the partition to which the request applies, it can handle the request directly; otherwise, it forwards the request to the appropriate node, receives the reply, and passes the reply along to the client.</p>
</li>
<li><p>Send all requests from clients to a routing tier first, which determines the node that should handle each request and forwards it accordingly. This routing tier does not itself handle any requests; it only acts as a partition-aware load balancer.</p>
</li>
<li><p>Require that clients be aware of the partitioning and the assignment of partitions to nodes. In this case, a client can connect directly to the appropriate node, without any intermediary.</p>
</li>
</ol>
<p>In all cases, the key problem is: how does the component making the routing decision (which may be one of the nodes, or the routing tier, or the client) learn about changes in the assignment of partitions to nodes?</p>
<p>This is a challenging problem, because it is important that all participants agree— otherwise requests would be sent to the wrong nodes and not handled correctly.</p>
<p>Many distributed data systems rely on a separate coordination service such as ZooKeeper to keep track of this cluster metadata. Each node registers itself in ZooKeeper, and ZooKeeper maintains the authoritative mapping of partitions to nodes. Other actors, such as the routing tier or the partitioning-aware client, can subscribe to this information in ZooKeeper. Whenever a partition changes ownership, or a node is added or removed, ZooKeeper notifies the routing tier so that it can keep its routing information up to date.</p>
<p>Cassandra and Riak take a different approach: they use a gossip protocol among the nodes to disseminate any changes in cluster state. Requests can be sent to any node, and that node forwards them to the appropriate node for the requested partition.</p>
<h4 id="Parallel-Query-Execution"><a href="#Parallel-Query-Execution" class="headerlink" title="Parallel Query Execution"></a>Parallel Query Execution</h4><p>So far we have focused on very simple queries that read or write a single key.</p>
<p>However, <em>massively parallel processing</em> (MPP) relational database products, often used for analytics, are much more sophisticated in the types of queries they support. The MPP query optimizer breaks this complex query into a num‐ ber of execution stages and partitions, many of which can be executed in parallel on different nodes of the database cluster. </p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>…</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://rezelchen.github.io/2020/08/08/DDIA5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/images/avatar.gif">
      <meta itemprop="name" content="Ray Chen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ray Chen's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/2020/08/08/DDIA5/" class="post-title-link" itemprop="url">Note for DDIA in Chapter 5</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-08-08 20:23:00" itemprop="dateCreated datePublished" datetime="2020-08-08T20:23:00+00:00">2020-08-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-08-20 07:33:40" itemprop="dateModified" datetime="2020-08-20T07:33:40+00:00">2020-08-20</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Replication"><a href="#Replication" class="headerlink" title="Replication"></a>Replication</h2><p>There are several reasons why you might want to replicate data:</p>
<ul>
<li><p>To keep data geographically close to your users (and thus reduce latency)</p>
</li>
<li><p>To allow the system to continue working even if some of its parts have failed (and thus increase availability)</p>
</li>
<li><p>To scale out the number of machines that can serve read queries (and thus increase read throughput)</p>
<p>We will discuss three popular algorithms for replicating changes between nodes: <em>single-leader</em>, <em>multi-leader</em>, and <em>leaderless</em> replication. Almost all distributed databases use one of these three approaches. </p>
</li>
</ul>
<h3 id="Leaders-and-Followers"><a href="#Leaders-and-Followers" class="headerlink" title="Leaders and Followers"></a>Leaders and Followers</h3><h4 id="Synchronous-Versus-Asynchronous-Replication"><a href="#Synchronous-Versus-Asynchronous-Replication" class="headerlink" title="Synchronous Versus Asynchronous Replication"></a>Synchronous Versus Asynchronous Replication</h4><p>In practice, if you enable synchronous replication on a database, it usually means that <em>one</em> of the followers is synchronous, and the others are asynchronous.</p>
<h4 id="Setting-Up-New-Followers"><a href="#Setting-Up-New-Followers" class="headerlink" title="Setting Up New Followers"></a>Setting Up New Followers</h4><h4 id="Handling-Node-Outages"><a href="#Handling-Node-Outages" class="headerlink" title="Handling Node Outages"></a>Handling Node Outages</h4><h5 id="Follower-failure-Catch-up-recovery"><a href="#Follower-failure-Catch-up-recovery" class="headerlink" title="Follower failure: Catch-up recovery"></a>Follower failure: Catch-up recovery</h5><h5 id="Leader-failure-Failover"><a href="#Leader-failure-Failover" class="headerlink" title="Leader failure: Failover"></a>Leader failure: Failover</h5><p>An automatic failover process usually consists of the following steps:</p>
<ul>
<li><p><em>Determining that the leader has failed.</em></p>
</li>
<li><p><em>Choosing a new leader.</em> The best candidate for leadership is usually the replica with the most up-to-date data changes from the old leader (to minimize any data loss).</p>
</li>
<li><p><em>Reconfiguring the system to use the new leader.</em></p>
</li>
</ul>
<p>Failover is fraught with things that can go wrong:</p>
<ul>
<li><p>If asynchronous replication is used, the new leader may not have received all the writes from the old leader before it failed. If the former leader rejoins the cluster after a new leader has been chosen, what should happen to those writes? The new leader may have received conflicting writes in the meantime. The most common solution is for the old leader’s unreplicated writes to simply be discarded, which may violate clients’ durability expectations.</p>
</li>
<li><p>Discarding writes is especially dangerous if other storage systems outside of the database need to be coordinated with the database contents. </p>
</li>
<li><p>In certain fault scenarios (see Chapter 8), it could happen that two nodes both believe that they are the leader. This situation is called split brain, and it is dan‐ gerous: if both leaders accept writes, and there is no process for resolving con‐ flicts (see “Multi-Leader Replication” on page 168), data is likely to be lost or corrupted. </p>
</li>
<li><p>What is the right timeout before the leader is declared dead? </p>
</li>
</ul>
<h4 id="Implementation-of-Replication-Logs"><a href="#Implementation-of-Replication-Logs" class="headerlink" title="Implementation of Replication Logs"></a>Implementation of Replication Logs</h4><h5 id="Statement-based-replication"><a href="#Statement-based-replication" class="headerlink" title="Statement-based replication"></a>Statement-based replication</h5><p>In the simplest case, the leader logs every write request (statement) that it executes and sends that statement log to its followers. </p>
<p>Although this may sound reasonable, there are various ways in which this approach to replication can break down:</p>
<ul>
<li><p>Any statement that calls a nondeterministic function, such as NOW() to get the current date and time or RAND() to get a random number, is likely to generate a different value on each replica.</p>
</li>
<li><p>If statements use an autoincrementing column, or if they depend on the existing data in the database (e.g., UPDATE … WHERE <some condition>), they must be executed in exactly the same order on each replica, or else they may have a differ‐ ent effect. This can be limiting when there are multiple concurrently executing transactions.</p>
</li>
<li><p>Statements that have side effects (e.g., triggers, stored procedures, user-defined functions) may result in different side effects occurring on each replica, unless the side effects are absolutely deterministic.</p>
</li>
</ul>
<h5 id="Write-ahead-log-WAL-shipping"><a href="#Write-ahead-log-WAL-shipping" class="headerlink" title="Write-ahead log (WAL) shipping"></a>Write-ahead log (WAL) shipping</h5><p>The main disadvantage is that the log describes the data on a very low level: a WAL con‐ tains details of which bytes were changed in which disk blocks. This makes replica‐ tion closely coupled to the storage engine. </p>
<p>That may seem like a minor implementation detail, but it can have a big operational impact. If the replication protocol allows the follower to use a newer software version than the leader, you can perform a zero-downtime upgrade of the database software by first upgrading the followers and then performing a failover to make one of the upgraded nodes the new leader. If the replication protocol does not allow this version mismatch, as is often the case with WAL shipping, such upgrades require downtime.</p>
<h5 id="Logical-row-based-log-replication"><a href="#Logical-row-based-log-replication" class="headerlink" title="Logical (row-based) log replication"></a>Logical (row-based) log replication</h5><p>An alternative is to use different log formats for replication and for the storage engine, which allows the replication log to be decoupled from the storage engine internals. This kind of replication log is called a logical log, to distinguish it from the storage engine’s (physical) data representation.</p>
<p>A logical log for a relational database is usually a sequence of records describing writes to database tables at the granularity of a row:</p>
<ul>
<li><p>For an inserted row, the log contains the new values of all columns.</p>
</li>
<li><p>For a deleted row, the log contains enough information to uniquely identify the row that was deleted. Typically this would be the primary key, but if there is no primary key on the table, the old values of all columns need to be logged.</p>
</li>
<li><p>For an updated row, the log contains enough information to uniquely identify the updated row, and the new values of all columns (or at least the new values of all columns that changed).</p>
</li>
</ul>
<p>A transaction that modifies several rows generates several such log records, followed by a record indicating that the transaction was committed.</p>
<h5 id="Trigger-based-replication"><a href="#Trigger-based-replication" class="headerlink" title="Trigger-based replication"></a>Trigger-based replication</h5><h3 id="Problems-with-Replication-Lag"><a href="#Problems-with-Replication-Lag" class="headerlink" title="Problems with Replication Lag"></a>Problems with Replication Lag</h3><h4 id="Reading-Your-Own-Writes"><a href="#Reading-Your-Own-Writes" class="headerlink" title="Reading Your Own Writes"></a>Reading Your Own Writes</h4><p><em>read-after-write consistency</em><br><em>cross-device read-after-write consistency</em></p>
<h4 id="Monotonic-Reads"><a href="#Monotonic-Reads" class="headerlink" title="Monotonic Reads"></a>Monotonic Reads</h4><p>When you read data, you may see an old value; monotonic reads only means that if one user makes several reads in sequence, they will not see time go backward— i.e., they will not read older data after having previously read newer data.</p>
<p>One way of achieving monotonic reads is to make sure that each user always makes their reads from the same replica.</p>
<h4 id="Consistent-Prefix-Reads"><a href="#Consistent-Prefix-Reads" class="headerlink" title="Consistent Prefix Reads"></a>Consistent Prefix Reads</h4><p>This guarantee says that if a sequence of writes happens in a certain order, then anyone reading those writes will see them appear in the same order.</p>
<h4 id="Solutions-for-Replication-Lag"><a href="#Solutions-for-Replication-Lag" class="headerlink" title="Solutions for Replication Lag"></a>Solutions for Replication Lag</h4><h3 id="Multi-Leader-Replication"><a href="#Multi-Leader-Replication" class="headerlink" title="Multi-Leader Replication"></a>Multi-Leader Replication</h3><h4 id="Use-Cases-for-Multi-Leader-Replication"><a href="#Use-Cases-for-Multi-Leader-Replication" class="headerlink" title="Use Cases for Multi-Leader Replication"></a>Use Cases for Multi-Leader Replication</h4><h5 id="Multi-datacenter-operation"><a href="#Multi-datacenter-operation" class="headerlink" title="Multi-datacenter operation"></a>Multi-datacenter operation</h5><p>Although multi-leader replication has advantages, it also has a big downside: the same data may be concurrently modified in two different datacenters, and those write conflicts must be resolved.</p>
<h5 id="Clients-with-offline-operation"><a href="#Clients-with-offline-operation" class="headerlink" title="Clients with offline operation"></a>Clients with offline operation</h5><p>Another situation in which multi-leader replication is appropriate is if you have an application that needs to continue to work while it is disconnected from the internet.</p>
<p>In this case, every device has a local database that acts as a leader (it accepts write requests), and there is an asynchronous multi-leader replication process (sync) between the replicas of your calendar on all of your devices. The replication lag may be hours or even days, depending on when you have internet access available.</p>
<p>There are tools that aim to make this kind of multi-leader configuration easier. For example, CouchDB is designed for this mode of operation.</p>
<h5 id="Collaborative-editing"><a href="#Collaborative-editing" class="headerlink" title="Collaborative editing"></a>Collaborative editing</h5><p><em>Real-time collaborative editing</em> applications allow several people to edit a document simultaneously. </p>
<p>We don’t usually think of collaborative editing as a database replication problem, but it has a lot in common with the previously mentioned offline editing use case. </p>
<h4 id="Handling-Write-Conflicts"><a href="#Handling-Write-Conflicts" class="headerlink" title="Handling Write Conflicts"></a>Handling Write Conflicts</h4><h5 id="Synchronous-versus-asynchronous-conflict-detection"><a href="#Synchronous-versus-asynchronous-conflict-detection" class="headerlink" title="Synchronous versus asynchronous conflict detection"></a>Synchronous versus asynchronous conflict detection</h5><p>In a single-leader database, the second writer will either block and wait for the first write to complete, or abort the second write transaction, forcing the user to retry the write. On the other hand, in a multi-leader setup, both writes are successful, and the conflict is only detected asynchronously at some later point in time. At that time, it may be too late to ask the user to resolve the conflict.</p>
<h5 id="Conflict-avoidance"><a href="#Conflict-avoidance" class="headerlink" title="Conflict avoidance"></a>Conflict avoidance</h5><p>The simplest strategy for dealing with conflicts is to avoid them: if the application can ensure that all writes for a particular record go through the same leader, then con‐ flicts cannot occur. </p>
<h5 id="Converging-toward-a-consistent-state"><a href="#Converging-toward-a-consistent-state" class="headerlink" title="Converging toward a consistent state"></a>Converging toward a consistent state</h5><p>That is not acceptable—every replication scheme must ensure that the data is eventually the same in all replicas. Thus, the database must resolve the conflict in a <em>convergent</em> way, which means that all replicas must arrive at the same final value when all changes have been replicated.</p>
<p>There are various ways of achieving convergent conflict resolution:</p>
<ul>
<li><p>Give each write a unique ID (e.g., a timestamp, a long random number, a UUID, or a hash of the key and value), pick the write with the highest ID as the <em>winner</em>, and throw away the other writes. If a timestamp is used, this technique is known as <em>last write wins</em> (LWW). Although this approach is popular, it is dangerously prone to data loss.</p>
</li>
<li><p>Give each replica a unique ID, and let writes that originated at a higher- numbered replica always take precedence over writes that originated at a lower- numbered replica. This approach also implies data loss.</p>
</li>
<li><p>Somehow merge the values together—e.g., order them alphabetically and then concatenate them.</p>
</li>
<li><p>Record the conflict in an explicit data structure that preserves all information, and write application code that resolves the conflict at some later time.</p>
</li>
</ul>
<h5 id="Custom-conflict-resolution-logic"><a href="#Custom-conflict-resolution-logic" class="headerlink" title="Custom conflict resolution logic"></a>Custom conflict resolution logic</h5><p>That code may be executed on write or on read:</p>
<p>On write: As soon as the database system detects a conflict in the log of replicated changes, it calls the conflict handler.</p>
<p>On read: When a conflict is detected, all the conflicting writes are stored. The next time the data is read, these multiple versions of the data are returned to the application. The application may prompt the user or automatically resolve the conflict, and write the result back to the database. </p>
<p>There has been some interesting research into automatically resolving conflicts caused by concurrent data modifications. A few lines of research are worth mentioning:</p>
<ul>
<li><p><em>Conflict-free replicated datatypes</em> (CRDTs) are a family of data structures for sets, maps, ordered lists, counters, etc. that can be concurrently edited by multiple users, and which automatically resolve conflicts in sensible ways. Some CRDTs have been implemented in Riak 2.0.</p>
</li>
<li><p><em>Mergeable persistent data structures</em> track history explicitly, similarly to the Git version control system, and use a three-way merge function (whereas CRDTs use two-way merges).</p>
</li>
<li><p><em>Operational transformation</em> is the conflict resolution algorithm behind col‐ laborative editing applications such as Etherpad and Google Docs. It was designed particularly for concurrent editing of an ordered list of items, such as the list of characters that constitute a text document.</p>
</li>
</ul>
<h5 id="What-is-a-conflict"><a href="#What-is-a-conflict" class="headerlink" title="What is a conflict?"></a>What is a conflict?</h5><h4 id="Multi-Leader-Replication-Topologies"><a href="#Multi-Leader-Replication-Topologies" class="headerlink" title="Multi-Leader Replication Topologies"></a>Multi-Leader Replication Topologies</h4><p>A problem with <em>circular and star topologies</em> is that if just one node fails, it can inter‐ rupt the flow of replication messages between other nodes, causing them to be unable to communicate until the node is fixed.</p>
<p>On the other hand, <em>all-to-all topologies</em> can have issues too. In particular, some network links may be faster than others (e.g., due to network congestion), with the result that some replication messages may “overtake” others.</p>
<h3 id="Leaderless-Replication"><a href="#Leaderless-Replication" class="headerlink" title="Leaderless Replication"></a>Leaderless Replication</h3><p>It once again became a fashiona‐ ble architecture for databases after Amazon used it for its in-house Dynamo system. Riak, Cassandra, and Voldemort are open source datastores with leaderless replication models inspired by Dynamo, so this kind of database is also known as Dynamo-style.</p>
<h4 id="Writing-to-the-Database-When-a-Node-Is-Down"><a href="#Writing-to-the-Database-When-a-Node-Is-Down" class="headerlink" title="Writing to the Database When a Node Is Down"></a>Writing to the Database When a Node Is Down</h4><h5 id="Read-repair-and-anti-entropy"><a href="#Read-repair-and-anti-entropy" class="headerlink" title="Read repair and anti-entropy"></a>Read repair and anti-entropy</h5><p>Two mechanisms are often used in Dynamo-style datastores:</p>
<ul>
<li>Read repair</li>
<li>Anti-entropy process</li>
</ul>
<p>Not all systems implement both of these; for example, Voldemort currently does not have an anti-entropy process. Note that without an anti-entropy process, values that are rarely read may be missing from some replicas and thus have reduced durability, because read repair is only performed when a value is read by the application.</p>
<h5 id="Quorums-for-reading-and-writing"><a href="#Quorums-for-reading-and-writing" class="headerlink" title="Quorums for reading and writing"></a>Quorums for reading and writing</h5><p>More generally, if there are n replicas, every write must be confirmed by w nodes to be considered successful, and we must query at least r nodes for each read. (In our example, n = 3, w = 2, r = 2.)</p>
<p>You can think of r and w as the minimum number of votes required for the read or write to be valid.</p>
<h4 id="Limitations-of-Quorum-Consistency"><a href="#Limitations-of-Quorum-Consistency" class="headerlink" title="Limitations of Quorum Consistency"></a>Limitations of Quorum Consistency</h4><p>However, even with w + r &gt; n, there are likely to be edge cases where stale values are returned. These depend on the implementation, but possible scenarios include:</p>
<ul>
<li><p>If a sloppy quorum is used, the w writes may end up on different nodes than the r reads, so there is no longer a guaranteed overlap between the r nodes and the w nodes.</p>
</li>
<li><p>If two writes occur concurrently, it is not clear which one happened first. In this case, the only safe solution is to merge the concurrent writes.</p>
</li>
<li><p>If a write happens concurrently with a read, the write may be reflected on only some of the replicas. In this case, it’s undetermined whether the read returns the old or the new value.</p>
</li>
<li><p>If a write succeeded on some replicas but failed on others (for example because the disks on some nodes are full), and overall succeeded on fewer than w replicas, it is not rolled back on the replicas where it succeeded. This means that if a write was reported as failed, subsequent reads may or may not return the value from that write.</p>
</li>
<li><p>If a node carrying a new value fails, and its data is restored from a replica carrying an old value, the number of replicas storing the new value may fall below w, breaking the quorum condition.</p>
</li>
<li><p>Even if everything is working correctly, there are edge cases in which you can get unlucky with the timing, as we shall see in “Linearizability and quorums” on page 334.</p>
</li>
</ul>
<p>In particular, you usually do not get the guarantees discussed in “Problems with Replication Lag” on page 161 (reading your writes, monotonic reads, or consistent prefix reads), so the previously mentioned anomalies can occur in applications. Stronger guarantees generally require transactions or consensus. </p>
<h5 id="Monitoring-staleness"><a href="#Monitoring-staleness" class="headerlink" title="Monitoring staleness"></a>Monitoring staleness</h5><h4 id="Sloppy-Quorums-and-Hinted-Handoff"><a href="#Sloppy-Quorums-and-Hinted-Handoff" class="headerlink" title="Sloppy Quorums and Hinted Handoff"></a>Sloppy Quorums and Hinted Handoff</h4><p><em>Sloppy quorum</em>: writes and reads still require w and r successful responses, but those may include nodes that are not among the designated n “home” nodes for a value. </p>
<p>Once the network interruption is fixed, any writes that one node temporarily accepted on behalf of another node are sent to the appropriate “home” nodes. This is called <em>hinted handoff</em>.</p>
<p>Sloppy quorums are particularly useful for increasing write availability: as long as any w nodes are available, the database can accept writes. However, this means that even when w + r &gt; n, you cannot be sure to read the latest value for a key, because the latest value may have been temporarily written to some nodes outside of n.</p>
<p>Thus, a sloppy quorum actually isn’t a quorum at all in the traditional sense. It’s only an assurance of durability, namely that the data is stored on w nodes somewhere. There is no guarantee that a read of r nodes will see it until the hinted handoff has completed.</p>
<h5 id="Multi-datacenter-operation-1"><a href="#Multi-datacenter-operation-1" class="headerlink" title="Multi-datacenter operation"></a>Multi-datacenter operation</h5><h4 id="Detecting-Concurrent-Writes"><a href="#Detecting-Concurrent-Writes" class="headerlink" title="Detecting Concurrent Writes"></a>Detecting Concurrent Writes</h4><p>Dynamo-style databases allow several clients to concurrently write to the same key, which means that conflicts will occur even if strict quorums are used.</p>
<p>The problem is that events may arrive in a different order at different nodes, due to variable network delays and partial failures. </p>
<h5 id="Last-write-wins-discarding-concurrent-writes"><a href="#Last-write-wins-discarding-concurrent-writes" class="headerlink" title="Last write wins (discarding concurrent writes)"></a>Last write wins (discarding concurrent writes)</h5><p>One approach for achieving eventual convergence is to declare that each replica need only store the most “recent” value and allow “older” values to be overwritten and dis‐ carded. Then, as long as we have some way of unambiguously determining which write is more “recent,” and every write is eventually copied to every replica, the repli‐ cas will eventually converge to the same value.</p>
<p>Even though the writes don’t have a natural ordering, we can force an arbitrary order on them. For example, we can attach a timestamp to each write, pick the biggest timestamp as the most “recent,” and discard any writes with an earlier timestamp.</p>
<h5 id="The-“happens-before”-relationship-and-concurrency"><a href="#The-“happens-before”-relationship-and-concurrency" class="headerlink" title="The “happens-before” relationship and concurrency"></a>The “happens-before” relationship and concurrency</h5><p>An operation A <em>happens before</em> another operation B if B knows about A, or depends on A, or builds upon A in some way. Whether one operation happens before another operation is the key to defining what concurrency means. In fact, we can simply say that two operations are concurrent if neither happens before the other (i.e., neither knows about the other)</p>
<h5 id="Capturing-the-happens-before-relationship"><a href="#Capturing-the-happens-before-relationship" class="headerlink" title="Capturing the happens-before relationship"></a>Capturing the happens-before relationship</h5><p>Note that the server can determine whether two operations are concurrent by looking at the version numbers—it does not need to interpret the value itself (so the value could be any data structure). The algorithm works as follows:</p>
<ul>
<li><p>The server maintains a version number for every key, increments the version number every time that key is written, and stores the new version number along with the value written.</p>
</li>
<li><p>When a client reads a key, the server returns all values that have not been overwritten, as well as the latest version number. A client must read a key before writing.</p>
</li>
<li><p>When a client writes a key, it must include the version number from the prior read, and it must merge together all values that it received in the prior read. (The response from a write request can be like a read, returning all current values, which allows us to chain several writes like in the shopping cart example.)</p>
</li>
<li><p>When the server receives a write with a particular version number, it can over‐ write all values with that version number or below (since it knows that they have been merged into the new value), but it must keep all values with a higher ver‐ sion number (because those values are concurrent with the incoming write).</p>
</li>
</ul>
<p>When a write includes the version number from a prior read, that tells us which pre‐ vious state the write is based on. If you make a write without including a version number, it is concurrent with all other writes, so it will not overwrite anything—it will just be returned as one of the values on subsequent reads.</p>
<h5 id="Merging-concurrently-written-values"><a href="#Merging-concurrently-written-values" class="headerlink" title="Merging concurrently written values"></a>Merging concurrently written values</h5><p>Merging sibling values is essentially the same problem as conflict resolution in multi- leader replication, which we discussed previously (see “Handling Write Conflicts” on page 171). A simple approach is to just pick one of the values based on a version number or timestamp (last write wins), but that implies losing data. So, you may need to do something more intelligent in application code.</p>
<p>However, if you want to allow people to also remove things from their carts, and not just add things, then taking the union of siblings may not yield the right result.</p>
<p>To prevent this problem, an item cannot simply be deleted from the database when it is removed; instead, the system must leave a marker with an appropriate version number to indicate that the item has been removed when merging siblings. Such a deletion marker is known as a <em>tombstone</em>.</p>
<h5 id="Version-vectors"><a href="#Version-vectors" class="headerlink" title="Version vectors"></a>Version vectors</h5><p>The collection of version numbers from all the replicas is called a version vector.</p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>…</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://rezelchen.github.io/2020/07/28/DDIA4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/images/avatar.gif">
      <meta itemprop="name" content="Ray Chen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ray Chen's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/2020/07/28/DDIA4/" class="post-title-link" itemprop="url">Note for DDIA in Chapter 4</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-07-28 17:38:00" itemprop="dateCreated datePublished" datetime="2020-07-28T17:38:00+00:00">2020-07-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-08-20 07:33:40" itemprop="dateModified" datetime="2020-08-20T07:33:40+00:00">2020-08-20</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Encoding-and-Evolution"><a href="#Encoding-and-Evolution" class="headerlink" title="Encoding and Evolution"></a>Encoding and Evolution</h2><h3 id="Formats-for-Encoding-Data"><a href="#Formats-for-Encoding-Data" class="headerlink" title="Formats for Encoding Data"></a>Formats for Encoding Data</h3><h4 id="Language-Specific-Formats"><a href="#Language-Specific-Formats" class="headerlink" title="Language-Specific Formats"></a>Language-Specific Formats</h4><p>It’s generally a bad idea to use your language’s built-in encoding for anything other than very transient purposes.</p>
<h4 id="JSON-XML-and-Binary-Variants"><a href="#JSON-XML-and-Binary-Variants" class="headerlink" title="JSON, XML, and Binary Variants"></a>JSON, XML, and Binary Variants</h4><h5 id="Binary-encoding"><a href="#Binary-encoding" class="headerlink" title="Binary encoding"></a>Binary encoding</h5><h4 id="Thrift-and-Protocol-Buffers"><a href="#Thrift-and-Protocol-Buffers" class="headerlink" title="Thrift and Protocol Buffers"></a>Thrift and Protocol Buffers</h4><p>…</p>
<h4 id="Avro"><a href="#Avro" class="headerlink" title="Avro"></a>Avro</h4><p>…</p>
<h4 id="The-Merits-of-Schemas"><a href="#The-Merits-of-Schemas" class="headerlink" title="The Merits of Schemas"></a>The Merits of Schemas</h4><ul>
<li><p>They can be much more compact than the various “binary JSON” variants, since they can omit field names from the encoded data.</p>
</li>
<li><p>The schema is a valuable form of documentation, and because the schema is required for decoding, you can be sure that it is up to date (whereas manually maintained documentation may easily diverge from reality).</p>
</li>
<li><p>Keeping a database of schemas allows you to check forward and backward com‐ patibility of schema changes, before anything is deployed.</p>
</li>
<li><p>For users of statically typed programming languages, the ability to generate code from the schema is useful, since it enables type checking at compile time.</p>
</li>
</ul>
<p>In summary, schema evolution allows the same kind of flexibility as schemaless/schema-on-read JSON databases provide, while also providing better guarantees about your data and better tooling.</p>
<h3 id="Modes-of-Dataflow"><a href="#Modes-of-Dataflow" class="headerlink" title="Modes of Dataflow"></a>Modes of Dataflow</h3><h4 id="Dataflow-Through-Databases"><a href="#Dataflow-Through-Databases" class="headerlink" title="Dataflow Through Databases"></a>Dataflow Through Databases</h4><h5 id="Different-values-written-at-different-times"><a href="#Different-values-written-at-different-times" class="headerlink" title="Different values written at different times"></a>Different values written at different times</h5><p>Schema evolution thus allows the entire database to appear as if it was encoded with a single schema, even though the underlying storage may contain records encoded with various historical versions of the schema.</p>
<h5 id="Archival-storage"><a href="#Archival-storage" class="headerlink" title="Archival storage"></a>Archival storage</h5><h4 id="Dataflow-Through-Services-REST-and-RPC"><a href="#Dataflow-Through-Services-REST-and-RPC" class="headerlink" title="Dataflow Through Services: REST and RPC"></a>Dataflow Through Services: REST and RPC</h4><h5 id="Web-services"><a href="#Web-services" class="headerlink" title="Web services"></a>Web services</h5><h5 id="The-problems-with-remote-procedure-calls-RPCs"><a href="#The-problems-with-remote-procedure-calls-RPCs" class="headerlink" title="The problems with remote procedure calls (RPCs)"></a>The problems with remote procedure calls (RPCs)</h5><p>Although RPC seems convenient at first, the approach is fundamentally flawed. A network request is very different from a local function call:</p>
<ul>
<li><p>A local function call is predictable and either succeeds or fails, depending only on parameters that are under your control. A network request is unpredictable: the request or response may be lost due to a network problem, or the remote machine may be slow or unavailable, and such problems are entirely outside of your control. </p>
</li>
<li><p>A network request has another possible outcome: it may return without a result, due to a timeout. </p>
</li>
<li><p>If you retry a failed network request, it could happen that the requests are actually getting through, and only the responses are getting lost. In that case, retrying will cause the action to be performed multiple times, unless you build a mechanism for deduplication (idempotence) into the protocol.</p>
</li>
<li><p>A network request is much slower than a function call, and its latency is also wildly variable.</p>
</li>
<li><p>When you call a local function, you can efficiently pass it references (pointers) to objects in local memory. When you make a network request, all those parameters need to be encoded into a sequence of bytes that can be sent over the network.</p>
</li>
<li><p>The client and the service may be implemented in different programming lan‐ guages, so the RPC framework must translate datatypes from one language into another.</p>
</li>
</ul>
<h5 id="Current-directions-for-RPC"><a href="#Current-directions-for-RPC" class="headerlink" title="Current directions for RPC"></a>Current directions for RPC</h5><p>The main focus of RPC frameworks is on requests between services owned by the same organization, typically within the same datacenter.</p>
<h5 id="Data-encoding-and-evolution-for-RPC"><a href="#Data-encoding-and-evolution-for-RPC" class="headerlink" title="Data encoding and evolution for RPC"></a>Data encoding and evolution for RPC</h5><p>We can make a simplifying assumption in the case of dataflow through services: it is reasonable to assume that all the servers will be updated first, and all the clients second. Thus, you only need backward compatibility on requests, and forward compatibility on responses.</p>
<p>If a compatibility-breaking change is required, the service provider often ends up maintaining multiple versions of the service API side by side.</p>
<h4 id="Message-Passing-Dataflow"><a href="#Message-Passing-Dataflow" class="headerlink" title="Message-Passing Dataflow"></a>Message-Passing Dataflow</h4><p>Using a message broker has several advantages compared to direct RPC:</p>
<ul>
<li><p>It can act as a buffer if the recipient is unavailable or overloaded, and thus improve system reliability.</p>
</li>
<li><p>It can automatically redeliver messages to a process that has crashed, and thus prevent messages from being lost.</p>
</li>
<li><p>It avoids the sender needing to know the IP address and port number of the recipient.</p>
</li>
<li><p>It allows one message to be sent to several recipients.</p>
</li>
<li><p>It logically decouples the sender from the recipient.</p>
</li>
</ul>
<p>However, a difference compared to RPC is that message-passing communication is usually one-way: a sender normally doesn’t expect to receive a reply to its messages. </p>
<h5 id="Message-brokers"><a href="#Message-brokers" class="headerlink" title="Message brokers"></a>Message brokers</h5><h5 id="Distributed-actor-frameworks"><a href="#Distributed-actor-frameworks" class="headerlink" title="Distributed actor frameworks"></a>Distributed actor frameworks</h5><p>The <em>actor model</em> is a programming model for concurrency in a single process. Each actor typically represents one client or entity, it may have some local state, and it communicates with other actors by sending and receiving asynchronous messages. Message delivery is not guaranteed: in certain error scenarios, mes‐ sages will be lost. Since each actor processes only one message at a time, it doesn’t need to worry about threads, and each actor can be scheduled independently by the framework.</p>
<p>In <em>distributed actor frameworks</em>, this programming model is used to scale an applica‐ tion across multiple nodes. The same message-passing mechanism is used, no matter whether the sender and recipient are on the same node or different nodes. </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://rezelchen.github.io/2020/07/24/DDIA3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/images/avatar.gif">
      <meta itemprop="name" content="Ray Chen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ray Chen's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/2020/07/24/DDIA3/" class="post-title-link" itemprop="url">Note for DDIA in Chapter 3</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-07-24 23:05:00" itemprop="dateCreated datePublished" datetime="2020-07-24T23:05:00+00:00">2020-07-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-08-20 07:33:40" itemprop="dateModified" datetime="2020-08-20T07:33:40+00:00">2020-08-20</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Storage-and-Retrieval"><a href="#Storage-and-Retrieval" class="headerlink" title="Storage and Retrieval"></a>Storage and Retrieval</h2><p>We will examine two families of storage engines: <em>log-structured</em> storage engines, and <em>page-oriented</em> storage engines such as B-trees.</p>
<h3 id="Data-Structures-That-Power-Your-Database"><a href="#Data-Structures-That-Power-Your-Database" class="headerlink" title="Data Structures That Power Your Database"></a>Data Structures That Power Your Database</h3><h4 id="Hash-Indexes"><a href="#Hash-Indexes" class="headerlink" title="Hash Indexes"></a>Hash Indexes</h4><p>Segments are never modified after they have been written, so the merged segment is written to a new file. The merging and compaction of frozen segments can be done in a background thread, and while it is going on, we can still continue to serve read and write requests as normal, using the old segment files. After the merging process is complete, we switch read requests to using the new merged segment instead of the old segments — and then the old segment files can simply be deleted.</p>
<p>Each segment now has its own in-memory hash table, mapping keys to file offsets. </p>
<p>An append-only design turns out to be good for several reasons:</p>
<ul>
<li><p>Appending and segment merging are sequential write operations, which are generally much faster than random writes, especially on magnetic spinning-disk hard drives.</p>
</li>
<li><p>Concurrency and crash recovery are much simpler if segment files are append-only or immutable.</p>
</li>
<li><p>Merging old segments avoids the problem of data files getting fragmented over time.</p>
</li>
</ul>
<p>The hash table index also has limitations:</p>
<ul>
<li>The hash table must fit in memory, so if you have a very large number of keys, you’re out of luck.</li>
<li>Range queries are not efficient. </li>
</ul>
<h4 id="SSTables-and-LSM-Trees"><a href="#SSTables-and-LSM-Trees" class="headerlink" title="SSTables and LSM-Trees"></a>SSTables and LSM-Trees</h4><p>SSTables have several big advantages over log segments with hash indexes:</p>
<ul>
<li><p>Merging segments is simple and efficient, even if the files are bigger than the available memory. The approach is like the one used in the <em>mergesort</em> algorithm</p>
</li>
<li><p>In order to find a particular key in the file, you no longer need to keep an index of all the keys in memory. You still need an in-memory index to tell you the offsets for some of the keys, but it can be sparse: one key for every few kilobytes of segment file is sufficient, because a few kilobytes can be scanned very quickly.i</p>
</li>
<li><p>Since read requests need to scan over several key-value pairs in the requested range anyway, it is possible to group those records into a block and compress it before writing it to disk. Each entry of the sparse in-memory index then points at the start of a compressed block.</p>
</li>
</ul>
<h5 id="Constructing-and-maintaining-SSTables"><a href="#Constructing-and-maintaining-SSTables" class="headerlink" title="Constructing and maintaining SSTables"></a>Constructing and maintaining SSTables</h5><ul>
<li><p>When a write comes in, add it to an in-memory balanced tree data structure (for<br>example, a red-black tree). This in-memory tree is sometimes called a memtable.</p>
</li>
<li><p>When the memtable gets bigger than some threshold—typically a few megabytes—write it out to disk as an SSTable file. This can be done efficiently because the tree already maintains the key-value pairs sorted by key. The new SSTable file becomes the most recent segment of the database. While the SSTable is being written out to disk, writes can continue to a new memtable instance.</p>
</li>
<li><p>In order to serve a read request, first try to find the key in the memtable, then in the most recent on-disk segment, then in the next-older segment, etc.</p>
</li>
<li><p>From time to time, run a merging and compaction process in the background to combine segment files and to discard overwritten or deleted values.</p>
</li>
</ul>
<p>This scheme works very well. It only suffers from one problem: if the database crashes, the most recent writes (which are in the memtable but not yet written out to disk) are lost. In order to avoid that problem, we can keep a separate log on disk to which every write is immediately appended, just like in the previous section. That log is not in sorted order, but that doesn’t matter, because its only purpose is to restore the memtable after a crash. Every time the memtable is written out to an SSTable, the corresponding log can be discarded.</p>
<h5 id="Making-an-LSM-tree-out-of-SSTables"><a href="#Making-an-LSM-tree-out-of-SSTables" class="headerlink" title="Making an LSM-tree out of SSTables"></a>Making an LSM-tree out of SSTables</h5><h5 id="Performance-optimizations"><a href="#Performance-optimizations" class="headerlink" title="Performance optimizations"></a>Performance optimizations</h5><p>The LSM-tree algorithm can be slow when looking up keys that do not exist in the database. In order to optimize this kind of access, storage engines often use additional <em>Bloom filters</em>.</p>
<p>There are also different strategies to determine the order and timing of how SSTables are compacted and merged. The most common options are <em>size-tiered</em> and <em>leveled</em> compaction.</p>
<h4 id="B-Trees"><a href="#B-Trees" class="headerlink" title="B-Trees"></a>B-Trees</h4><p>Like SSTables, B-trees keep key-value pairs sorted by key, which allows efficient key- value lookups and range queries.</p>
<p>B-trees break the database down into fixed-size blocks or pages, traditionally 4 KB in size (sometimes bigger), and read or write one page at a time. </p>
<p>Each page can be identified using an address or location, which allows one page to refer to another—similar to a pointer, but on disk instead of in memory. </p>
<h5 id="Making-B-trees-reliable"><a href="#Making-B-trees-reliable" class="headerlink" title="Making B-trees reliable"></a>Making B-trees reliable</h5><p>In order to make the database resilient to crashes, it is common for B-tree implemen‐ tations to include an additional data structure on disk: a write-ahead log (WAL, also known as a redo log). </p>
<p>An additional complication of updating pages in place is that careful concurrency control is required if multiple threads are going to access the B-tree at the same time —otherwise a thread may see the tree in an inconsistent state. This is typically done by protecting the tree’s data structures with <em>latches</em> (lightweight locks).</p>
<h5 id="B-tree-optimizations"><a href="#B-tree-optimizations" class="headerlink" title="B-tree optimizations"></a>B-tree optimizations</h5><ul>
<li><p>Instead of overwriting pages and maintaining a WAL for crash recovery, some databases (like LMDB) use a copy-on-write scheme. A modified page is written to a different location, and a new version of the parent pages in the tree is created, pointing at the new location. </p>
</li>
<li><p>We can save space in pages by not storing the entire key, but abbreviating it.</p>
</li>
<li><p>If a query needs to scan over a large part of the key range in sorted order, that page-by-page layout can be ineffi‐ cient, because a disk seek may be required for every page that is read. Many B- tree implementations therefore try to lay out the tree so that leaf pages appear in sequential order on disk. </p>
</li>
<li><p>Additional pointers have been added to the tree. For example, each leaf page may have references to its sibling pages to the left and right, which allows scanning keys in order without jumping back to parent pages.</p>
</li>
<li><p>B-tree variants such as fractal trees borrow some log-structured ideas to reduce disk seeks (and they have nothing to do with fractals).</p>
</li>
</ul>
<h4 id="Comparing-B-Trees-and-LSM-Trees"><a href="#Comparing-B-Trees-and-LSM-Trees" class="headerlink" title="Comparing B-Trees and LSM-Trees"></a>Comparing B-Trees and LSM-Trees</h4><h5 id="Advantages-of-LSM-trees"><a href="#Advantages-of-LSM-trees" class="headerlink" title="Advantages of LSM-trees"></a>Advantages of LSM-trees</h5><p>LSM-trees are typically able to sustain higher write throughput than B- trees.</p>
<p>LSM-trees can be compressed better, and thus often produce smaller files on disk than B-trees. </p>
<h5 id="Downsides-of-LSM-trees"><a href="#Downsides-of-LSM-trees" class="headerlink" title="Downsides of LSM-trees"></a>Downsides of LSM-trees</h5><p>A downside of log-structured storage is that the compaction process can sometimes interfere with the performance of ongoing reads and writes. </p>
<p>Another issue with compaction arises at high write throughput: the disk’s finite write bandwidth needs to be shared between the initial write (logging and flushing a memtable to disk) and the compaction threads running in the background.</p>
<p>If write throughput is high and compaction is not configured carefully, it can happen that compaction cannot keep up with the rate of incoming writes.</p>
<p>An advantage of B-trees is that each key exists in exactly one place in the index, whereas a log-structured storage engine may have multiple copies of the same key in different segments. This aspect makes B-trees attractive in databases that want to offer strong transactional semantics: in many relational databases, transaction isola‐ tion is implemented using locks on ranges of keys, and in a B-tree index, those locks can be directly attached to the tree </p>
<h4 id="Other-Indexing-Structures"><a href="#Other-Indexing-Structures" class="headerlink" title="Other Indexing Structures"></a>Other Indexing Structures</h4><h5 id="Storing-values-within-the-index"><a href="#Storing-values-within-the-index" class="headerlink" title="Storing values within the index"></a>Storing values within the index</h5><p>In some situations, the extra hop from the index to the heap file is too much of a performance penalty for reads, so it can be desirable to store the indexed row directly within an index. This is known as a <em>clustered index</em>.</p>
<h5 id="Multi-column-indexes"><a href="#Multi-column-indexes" class="headerlink" title="Multi-column indexes"></a>Multi-column indexes</h5><p>…</p>
<h3 id="Transaction-Processing-or-Analytics"><a href="#Transaction-Processing-or-Analytics" class="headerlink" title="Transaction Processing or Analytics?"></a>Transaction Processing or Analytics?</h3><p>online transaction processing(OLTP)<br>online analytic processing (OLAP)</p>
<h4 id="Data-Warehousing"><a href="#Data-Warehousing" class="headerlink" title="Data Warehousing"></a>Data Warehousing</h4><p>The data warehouse con‐ tains a read-only copy of the data in all the various OLTP systems in the company. Data is extracted from OLTP databases (using either a periodic data dump or a con‐ tinuous stream of updates), transformed into an analysis-friendly schema, cleaned up, and then loaded into the data warehouse. This process of getting data into the warehouse is known as Extract–Transform–Load (ETL)</p>
<h5 id="The-divergence-between-OLTP-databases-and-data-warehouses"><a href="#The-divergence-between-OLTP-databases-and-data-warehouses" class="headerlink" title="The divergence between OLTP databases and data warehouses"></a>The divergence between OLTP databases and data warehouses</h5><h4 id="Stars-and-Snowflakes-Schemas-for-Analytics"><a href="#Stars-and-Snowflakes-Schemas-for-Analytics" class="headerlink" title="Stars and Snowflakes: Schemas for Analytics"></a>Stars and Snowflakes: Schemas for Analytics</h4><p>…</p>
<h3 id="Column-Oriented-Storage"><a href="#Column-Oriented-Storage" class="headerlink" title="Column-Oriented Storage"></a>Column-Oriented Storage</h3><p>…</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://rezelchen.github.io/2020/07/24/DDIA2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/images/avatar.gif">
      <meta itemprop="name" content="Ray Chen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ray Chen's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/2020/07/24/DDIA2/" class="post-title-link" itemprop="url">Note for DDIA in Chapter 2</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-07-24 18:54:40" itemprop="dateCreated datePublished" datetime="2020-07-24T18:54:40+00:00">2020-07-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-08-20 07:33:40" itemprop="dateModified" datetime="2020-08-20T07:33:40+00:00">2020-08-20</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Data-Models-and-Query-Languages"><a href="#Data-Models-and-Query-Languages" class="headerlink" title="Data Models and Query Languages"></a>Data Models and Query Languages</h2><h3 id="Relational-Model-Versus-Document-Model"><a href="#Relational-Model-Versus-Document-Model" class="headerlink" title="Relational Model Versus Document Model"></a>Relational Model Versus Document Model</h3><h4 id="Many-to-One-and-Many-to-Many-Relationships"><a href="#Many-to-One-and-Many-to-Many-Relationships" class="headerlink" title="Many-to-One and Many-to-Many Relationships"></a>Many-to-One and Many-to-Many Relationships</h4><p>Removing such duplication is the key idea behind <em>normalization</em> in databases.</p>
<p>As a rule of thumb, if you’re duplicating values that could be stored in just one place, the schema is not normalized.</p>
<h4 id="Are-Document-Databases-Repeating-History"><a href="#Are-Document-Databases-Repeating-History" class="headerlink" title="Are Document Databases Repeating History?"></a>Are Document Databases Repeating History?</h4><h5 id="The-network-model"><a href="#The-network-model" class="headerlink" title="The network model"></a>The network model</h5><h5 id="The-relational-model"><a href="#The-relational-model" class="headerlink" title="The relational model"></a>The relational model</h5><h5 id="Comparison-to-document-databases"><a href="#Comparison-to-document-databases" class="headerlink" title="Comparison to document databases"></a>Comparison to document databases</h5><h4 id="Relational-Versus-Document-Databases-Today"><a href="#Relational-Versus-Document-Databases-Today" class="headerlink" title="Relational Versus Document Databases Today"></a>Relational Versus Document Databases Today</h4><h5 id="Which-data-model-leads-to-simpler-application-code"><a href="#Which-data-model-leads-to-simpler-application-code" class="headerlink" title="Which data model leads to simpler application code?"></a>Which data model leads to simpler application code?</h5><h5 id="Schema-flexibility-in-the-document-model"><a href="#Schema-flexibility-in-the-document-model" class="headerlink" title="Schema flexibility in the document model"></a>Schema flexibility in the document model</h5><p>A more accurate term is schema-on-read (the structure of the data is implicit, and only interpreted when the data is read), in contrast with schema-on-write (the traditional approach of relational databases, where the schema is explicit and the database ensures all written data conforms to it)</p>
<h5 id="Data-locality-for-queries"><a href="#Data-locality-for-queries" class="headerlink" title="Data locality for queries"></a>Data locality for queries</h5><h5 id="Convergence-of-document-and-relational-databases"><a href="#Convergence-of-document-and-relational-databases" class="headerlink" title="Convergence of document and relational databases"></a>Convergence of document and relational databases</h5><p>A hybrid of the relational and document models is a good route for databases to take in the future.</p>
<h3 id="Query-Languages-for-Data"><a href="#Query-Languages-for-Data" class="headerlink" title="Query Languages for Data"></a>Query Languages for Data</h3><p>A declarative query language is attractive because it is typically more concise and eas‐ ier to work with than an imperative API. But more importantly, it also hides imple‐ mentation details of the database engine, which makes it possible for the database system to introduce performance improvements without requiring any changes to queries.</p>
<h4 id="Declarative-Queries-on-the-Web"><a href="#Declarative-Queries-on-the-Web" class="headerlink" title="Declarative Queries on the Web"></a>Declarative Queries on the Web</h4><h4 id="MapReduce-Querying"><a href="#MapReduce-Querying" class="headerlink" title="MapReduce Querying"></a>MapReduce Querying</h4><p>MapReduce is a fairly low-level programming model for distributed execution on a cluster of machines. Higher-level query languages like SQL can be implemented as a pipeline of MapReduce operations but there are also many dis‐ tributed implementations of SQL that don’t use MapReduce.</p>
<p>Note there is nothing in SQL that constrains it to running on a single machine, and MapReduce doesn’t have a monopoly on distributed query execution.</p>
<h3 id="Graph-Like-Data-Models"><a href="#Graph-Like-Data-Models" class="headerlink" title="Graph-Like Data Models"></a>Graph-Like Data Models</h3><p>In the examples just given, all the vertices in a graph represent the same kind of thing (people, web pages, or road junctions, respectively). However, graphs are not limited to such homogeneous data: an equally powerful use of graphs is to provide a consis‐ tent way of storing completely different types of objects in a single datastore.</p>
<h4 id="Property-Graphs"><a href="#Property-Graphs" class="headerlink" title="Property Graphs"></a>Property Graphs</h4><p>properties of vertex:</p>
<ul>
<li>A unique identifier</li>
<li>A set of outgoing edges</li>
<li>A set of incoming edges</li>
<li>A collection of properties (key-value pairs)</li>
</ul>
<p>properties of edge:</p>
<ul>
<li>A unique identifier</li>
<li>The vertex at which the edge starts (the tail vertex)</li>
<li>The vertex at which the edge ends (the head vertex)</li>
<li>A label to describe the kind of relationship between the two vertices</li>
<li>A collection of properties (key-value pairs)</li>
</ul>
<p>You can think of a graph store as consisting of two relational tables, one for vertices and one for edges</p>
<h4 id="The-Cypher-Query-Language"><a href="#The-Cypher-Query-Language" class="headerlink" title="The Cypher Query Language"></a>The Cypher Query Language</h4><p>…</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://rezelchen.github.io/2020/05/27/DDIA1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/images/avatar.gif">
      <meta itemprop="name" content="Ray Chen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ray Chen's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/2020/05/27/DDIA1/" class="post-title-link" itemprop="url">Note for DDIA in Chapter 1</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-05-27 22:54:40" itemprop="dateCreated datePublished" datetime="2020-05-27T22:54:40+00:00">2020-05-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-08-20 07:33:40" itemprop="dateModified" datetime="2020-08-20T07:33:40+00:00">2020-08-20</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="Reliability"><a href="#Reliability" class="headerlink" title="Reliability"></a>Reliability</h3><ul>
<li>The application performs the function that the user expected.</li>
<li>It can tolerate the user making mistakes or using the software in unexpected ways.</li>
<li>Its performance is good enough for the required use case, under the expected load and data volume.</li>
<li>The system prevents any unauthorized access and abuse.</li>
</ul>
<h3 id="Scalability"><a href="#Scalability" class="headerlink" title="Scalability"></a>Scalability</h3><h4 id="Describing-Load"><a href="#Describing-Load" class="headerlink" title="Describing Load"></a>Describing Load</h4><p>Load can be described with a few numbers which we call <em>load parameters</em>. The best choice of parameters depends on the architecture of your system: it may be requests per second to a web server, the ratio of reads to writes in a database, the number of simultaneously active users in a chat room, the hit rate on a cache, or something else. Perhaps the average case is what matters for you, or perhaps your bottleneck is dominated by a small number of extreme cases.</p>
<h4 id="Describing-Performance"><a href="#Describing-Performance" class="headerlink" title="Describing Performance"></a>Describing Performance</h4><ul>
<li>throughput — the number of records we can process per second</li>
<li>the total time it takes to run a job on a dataset of a certain size</li>
<li>response time — the time between a client sending a request and receiving a response</li>
<li>latency is the duration that a request is waiting to be handled—during which it is <em>latent</em>, awaiting service</li>
</ul>
<p>High percentiles of response times, also known as <em>tail latencies</em>, are important because they directly affect users’ experience of the service.</p>
<h4 id="Approaches-for-Coping-with-Load"><a href="#Approaches-for-Coping-with-Load" class="headerlink" title="Approaches for Coping with Load"></a>Approaches for Coping with Load</h4><p>While distributing stateless services across multiple machines is fairly straightfor‐ ward, taking stateful data systems from a single node to a distributed setup can intro‐ duce a lot of additional complexity.</p>
<p>An architecture that scales well for a particular application is built around assumptions of which operations will be common and which will be rare — the load parameters.</p>
<p>Even though they are specific to a particular application, scalable architectures are nevertheless usually built from general-purpose building blocks, arranged in familiar patterns.</p>
<h3 id="Maintainability"><a href="#Maintainability" class="headerlink" title="Maintainability"></a>Maintainability</h3><h4 id="Operability-Making-Life-Easy-for-Operations"><a href="#Operability-Making-Life-Easy-for-Operations" class="headerlink" title="Operability: Making Life Easy for Operations"></a>Operability: Making Life Easy for Operations</h4><h4 id="Simplicity-Managing-Complexity"><a href="#Simplicity-Managing-Complexity" class="headerlink" title="Simplicity: Managing Complexity"></a>Simplicity: Managing Complexity</h4><h4 id="Evolvability-Making-Change-Easy"><a href="#Evolvability-Making-Change-Easy" class="headerlink" title="Evolvability: Making Change Easy"></a>Evolvability: Making Change Easy</h4>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  



        </div>
        

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ray Chen</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/blog/lib/anime.min.js"></script>

<script src="/blog/js/utils.js"></script>

<script src="/blog/js/motion.js"></script>


<script src="/blog/js/schemes/muse.js"></script>


<script src="/blog/js/next-boot.js"></script>


  















  

  

</body>
</html>
